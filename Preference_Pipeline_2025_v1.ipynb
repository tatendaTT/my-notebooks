{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476345c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Simulation Phase (Agent V5 - Deterministic Preference, Structured Graph NO NOISE) ---\n",
      "\n",
      "Generating structured graph for Seed 0...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 0...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 1...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 1...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 2...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 2...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 3...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 3...\n",
      " Starting simulations for Seed 3...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 3. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 4...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 4...\n",
      " Starting simulations for Seed 4...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 4. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 5...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 5...\n",
      " Starting simulations for Seed 5...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 5. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 6...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 6...\n",
      " Starting simulations for Seed 6...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 6. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 7...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 7...\n",
      " Starting simulations for Seed 7...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 7. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 8...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 8...\n",
      " Starting simulations for Seed 8...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 8. Completed: 250, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 9...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 9...\n",
      " Starting simulations for Seed 9...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 50 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 9. Completed: 250, Skipped: 0\n",
      "\n",
      "--- Simulation Phase Complete ---\n",
      "Generated 2500 records in 0.22s.\n",
      "\n",
      "--- Starting Clustering Phase ---\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 3...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 4...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 5...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 6...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 7...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 8...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 9...\n",
      "  BoC Features extracted with shape: (250, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "\n",
      "--- Clustering Phase Complete ---\n",
      "Completed in 0.49s.\n",
      "\n",
      "--- Clustering Summary Results ---\n",
      "   Seed Algorithm Feature_Metric  Num_Clusters_Target  Num_Clusters_Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Cluster_Color_Percentage  Silhouette     ARI     NMI  Max_Color_Separation\n",
      "0     0    kmeans    boc(cosine)                    5                   5      {1: {'blue': '71.43%', 'green': '7.43%', 'red': '7.43%', 'white': '7.71%', 'yellow': '6.00%'}, 2: {'blue': '7.16%', 'green': '13.09%', 'red': '64.20%', 'white': '8.40%', 'yellow': '7.16%'}, 3: {'blue': '6.95%', 'green': '69.52%', 'red': '8.56%', 'white': '6.42%', 'yellow': '8.56%'}, 4: {'blue': '10.93%', 'green': '11.48%', 'red': '10.93%', 'white': '8.38%', 'yellow': '58.29%'}, 5: {'blue': '7.14%', 'green': '9.52%', 'red': '11.11%', 'white': '63.49%', 'yellow': '8.73%'}}      0.8459  1.0000  1.0000               65.4286\n",
      "1     1    kmeans    boc(cosine)                    5                   5    {1: {'blue': '12.07%', 'green': '10.13%', 'red': '11.21%', 'white': '54.96%', 'yellow': '11.64%'}, 2: {'blue': '6.52%', 'green': '7.61%', 'red': '72.01%', 'white': '6.25%', 'yellow': '7.61%'}, 3: {'blue': '11.48%', 'green': '58.37%', 'red': '10.31%', 'white': '9.53%', 'yellow': '10.31%'}, 4: {'blue': '71.43%', 'green': '5.57%', 'red': '6.97%', 'white': '7.32%', 'yellow': '8.71%'}, 5: {'blue': '8.88%', 'green': '8.88%', 'red': '9.74%', 'white': '8.02%', 'yellow': '64.47%'}}      0.8551  1.0000  1.0000               65.8537\n",
      "2     2    kmeans    boc(cosine)                    5                   5         {1: {'blue': '4.53%', 'green': '79.29%', 'red': '7.12%', 'white': '5.83%', 'yellow': '3.24%'}, 2: {'blue': '8.60%', 'green': '6.30%', 'red': '75.93%', 'white': '5.44%', 'yellow': '3.72%'}, 3: {'blue': '9.02%', 'green': '8.52%', 'red': '8.77%', 'white': '9.77%', 'yellow': '63.91%'}, 4: {'blue': '9.67%', 'green': '6.84%', 'red': '14.86%', 'white': '58.96%', 'yellow': '9.67%'}, 5: {'blue': '63.00%', 'green': '10.72%', 'red': '9.38%', 'white': '8.85%', 'yellow': '8.04%'}}      0.8843  1.0000  1.0000               76.0518\n",
      "3     3    kmeans    boc(cosine)                    5                   5   {1: {'blue': '13.42%', 'green': '14.37%', 'red': '48.02%', 'white': '12.48%', 'yellow': '11.72%'}, 2: {'blue': '70.22%', 'green': '7.99%', 'red': '7.99%', 'white': '4.84%', 'yellow': '8.96%'}, 3: {'blue': '11.08%', 'green': '8.06%', 'red': '13.35%', 'white': '9.57%', 'yellow': '57.93%'}, 4: {'blue': '5.51%', 'green': '76.77%', 'red': '7.87%', 'white': '6.30%', 'yellow': '3.54%'}, 5: {'blue': '7.18%', 'green': '10.77%', 'red': '7.89%', 'white': '64.11%', 'yellow': '10.05%'}}      0.8439  0.9902  0.9878               73.2283\n",
      "4     4    kmeans    boc(cosine)                    5                   5      {1: {'blue': '11.34%', 'green': '59.72%', 'red': '12.75%', 'white': '6.68%', 'yellow': '9.51%'}, 2: {'blue': '14.73%', 'green': '8.51%', 'red': '12.03%', 'white': '10.79%', 'yellow': '53.94%'}, 3: {'blue': '7.45%', 'green': '2.13%', 'red': '6.03%', 'white': '76.24%', 'yellow': '8.16%'}, 4: {'blue': '75.00%', 'green': '5.62%', 'red': '5.94%', 'white': '5.94%', 'yellow': '7.50%'}, 5: {'blue': '5.48%', 'green': '4.52%', 'red': '77.42%', 'white': '6.45%', 'yellow': '6.13%'}}      0.8710  1.0000  1.0000               74.1135\n",
      "5     5    kmeans    boc(cosine)                    5                   5    {1: {'blue': '8.12%', 'green': '64.72%', 'red': '10.15%', 'white': '9.90%', 'yellow': '7.11%'}, 2: {'blue': '51.26%', 'green': '10.79%', 'red': '11.87%', 'white': '15.47%', 'yellow': '10.61%'}, 3: {'blue': '7.16%', 'green': '6.30%', 'red': '6.59%', 'white': '9.74%', 'yellow': '70.20%'}, 4: {'blue': '7.61%', 'green': '7.61%', 'red': '11.27%', 'white': '67.61%', 'yellow': '5.92%'}, 5: {'blue': '9.89%', 'green': '9.09%', 'red': '60.16%', 'white': '11.50%', 'yellow': '9.36%'}}      0.8402  1.0000  1.0000               63.8968\n",
      "6     6    kmeans    boc(cosine)                    5                   5    {1: {'blue': '48.74%', 'green': '12.94%', 'red': '9.58%', 'white': '12.44%', 'yellow': '16.30%'}, 2: {'blue': '3.54%', 'green': '5.14%', 'red': '3.22%', 'white': '2.89%', 'yellow': '85.21%'}, 3: {'blue': '6.91%', 'green': '8.78%', 'red': '7.71%', 'white': '65.16%', 'yellow': '11.44%'}, 4: {'blue': '11.64%', 'green': '9.79%', 'red': '52.91%', 'white': '10.85%', 'yellow': '14.81%'}, 5: {'blue': '4.36%', 'green': '77.88%', 'red': '5.30%', 'white': '5.61%', 'yellow': '6.85%'}}      0.8578  1.0000  1.0000               82.3151\n",
      "7     7    kmeans    boc(cosine)                    5                   5         {1: {'blue': '5.75%', 'green': '5.43%', 'red': '6.07%', 'white': '78.27%', 'yellow': '4.47%'}, 2: {'blue': '79.01%', 'green': '5.68%', 'red': '3.95%', 'white': '4.44%', 'yellow': '6.91%'}, 3: {'blue': '6.63%', 'green': '68.97%', 'red': '7.16%', 'white': '7.96%', 'yellow': '9.28%'}, 4: {'blue': '10.07%', 'green': '5.76%', 'red': '70.14%', 'white': '6.47%', 'yellow': '7.55%'}, 5: {'blue': '10.23%', 'green': '5.85%', 'red': '7.89%', 'white': '8.77%', 'yellow': '67.25%'}}      0.9008  1.0000  1.0000               75.0617\n",
      "8     8    kmeans    boc(cosine)                    5                   5   {1: {'blue': '7.74%', 'green': '68.11%', 'red': '7.74%', 'white': '7.12%', 'yellow': '9.29%'}, 2: {'blue': '10.84%', 'green': '10.63%', 'red': '13.50%', 'white': '8.79%', 'yellow': '56.24%'}, 3: {'blue': '13.14%', 'green': '7.42%', 'red': '9.75%', 'white': '61.44%', 'yellow': '8.26%'}, 4: {'blue': '82.68%', 'green': '4.72%', 'red': '3.94%', 'white': '5.51%', 'yellow': '3.15%'}, 5: {'blue': '11.13%', 'green': '12.89%', 'red': '49.80%', 'white': '14.45%', 'yellow': '11.72%'}}      0.8282  1.0000  1.0000               79.5276\n",
      "9     9    kmeans    boc(cosine)                    5                   5  {1: {'blue': '9.79%', 'green': '10.14%', 'red': '9.79%', 'white': '5.59%', 'yellow': '64.69%'}, 2: {'blue': '57.31%', 'green': '12.25%', 'red': '8.70%', 'white': '10.28%', 'yellow': '11.46%'}, 3: {'blue': '9.12%', 'green': '6.43%', 'red': '5.36%', 'white': '72.39%', 'yellow': '6.70%'}, 4: {'blue': '12.83%', 'green': '57.01%', 'red': '11.16%', 'white': '9.98%', 'yellow': '9.03%'}, 5: {'blue': '9.02%', 'green': '15.41%', 'red': '49.81%', 'white': '12.97%', 'yellow': '12.78%'}}      0.8496  1.0000  1.0000               67.0241\n",
      "--------------------------------------------------\n",
      "Avg ARI: 0.9990\n",
      "Avg NMI: 0.9988\n",
      "Avg Silhouette: 0.8577\n",
      "--------------------------------------------------\n",
      "Summary results saved to clustering_results_boc_kmeans_k5_agent_v5_structured_NO_NOISE\\summary_boc_kmeans_k5_agent_v5_structured_NO_NOISE.csv\n",
      "\n",
      "--- Remapping cluster labels for optimal visualization using Hungarian algorithm ---\n",
      "  Target Plot Order Mapping (Rows): {'blue': 1, 'green': 2, 'red': 3, 'white': 4, 'yellow': 5}\n",
      "  Predicted Labels Found for CM Cols: [1, 2, 3, 4, 5]\n",
      "  Raw Confusion Matrix (Rows: True Target Int, Cols: Based on True Target Int Range):\n",
      "[[108 237   0 131  47]\n",
      " [203   0 164  87  50]\n",
      " [ 52 158   0  79 197]\n",
      " [100   0 204  98  86]\n",
      " [ 38 160 146  64  91]]\n",
      "  Building Cost Matrix (True Rows vs Predicted Cols)...\n",
      "  Cost Matrix Shape: (5, 5)\n",
      "  Optimal Assignment (True Target Int -> Original Predicted Cluster Label):\n",
      "    True 'blue' (Target Int 1) best matches Original Cluster Label 2\n",
      "    True 'green' (Target Int 2) best matches Original Cluster Label 1\n",
      "    True 'red' (Target Int 3) best matches Original Cluster Label 5\n",
      "    True 'white' (Target Int 4) best matches Original Cluster Label 3\n",
      "    True 'yellow' (Target Int 5) best matches Original Cluster Label 4\n",
      "  Cluster Remapping Dict (Original Cluster -> Target Plot Int): {2: 1, 1: 2, 5: 3, 3: 4, 4: 5}\n",
      "\n",
      "--- Generating Confusion Matrix (using OPTIMALLY REMAPPED cluster labels) ---\n",
      "\n",
      "--- Confusion Matrix Generation ---\n",
      " Row Mapping (True Color -> Int): {'blue': 1, 'green': 2, 'red': 3, 'white': 4, 'yellow': 5}\n",
      " Col Labels (Predicted Clusters): [1, 2, 3, 4, 5]\n",
      " Saved confusion matrix plot to clustering_results_boc_kmeans_k5_agent_v5_structured_NO_NOISE\\plots_remapped_optimal\\confusion_matrix_Cluster_Remapped_vs_Color Preference.png\n",
      " Could not rename remapped plot: [WinError 183] Cannot create a file when that file already exists: 'clustering_results_boc_kmeans_k5_agent_v5_structured_NO_NOISE\\\\plots_remapped_optimal\\\\confusion_matrix_Cluster_Remapped_vs_Color Preference.png' -> 'clustering_results_boc_kmeans_k5_agent_v5_structured_NO_NOISE\\\\plots_remapped_optimal\\\\confusion_matrix_OPTIMAL_REORDERED_boc_kmeans_k5.png'\n",
      "\n",
      "Pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Imports\n",
    "# ==============================================================================\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Set, Tuple, Any, Optional\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack as sparse_hstack\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment # For optimal remapping\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Agent Definition\n",
    "# ==============================================================================\n",
    "class Agent:\n",
    "    \"\"\" Agent with Preference-Driven Goal: Find N preferred nodes using\n",
    "        Deterministic Preference strategy (always choose preferred if available). \"\"\"\n",
    "    def __init__(self, start_node: int, color_preference: str,\n",
    "                 target_preferred_count: int, max_steps: int):\n",
    "        self.start_node: int = start_node; self.color_preference: str = color_preference\n",
    "        self.target_preferred_count: int = target_preferred_count; self.max_steps: int = max_steps\n",
    "        self.current_node: int = start_node; self.visited_nodes: Set[int] = {start_node}\n",
    "        self.path: List[int] = [start_node]; self.preferred_nodes_visited: Set[int] = set()\n",
    "        self.visited_shapes: List[str] = []; self.visited_shininess: List[str] = []\n",
    "\n",
    "    # <<< EXPLICIT LOOP FIX applied to find_next_node >>>\n",
    "    def find_next_node(self, graph: nx.Graph, rng: random.Random) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Determines the next node using a DETERMINISTIC PREFERENCE strategy:\n",
    "        ALWAYS chooses randomly from unvisited neighbors matching preference, if any exist.\n",
    "        Otherwise, chooses randomly from non-matching unvisited neighbors.\n",
    "        (Using explicit loop for robustness against potential NameError).\n",
    "        \"\"\"\n",
    "        neighbors = list(graph.neighbors(self.current_node))\n",
    "        # Ensure nodes are valid graph nodes before checking visited status\n",
    "        valid_neighbors = [n for n in neighbors if n in graph]\n",
    "        unvisited_neighbors = [n for n in valid_neighbors if n not in self.visited_nodes]\n",
    "\n",
    "        if not unvisited_neighbors:\n",
    "            return None # Stuck\n",
    "\n",
    "        preferred_unvisited_neighbors = []\n",
    "        non_preferred_unvisited_neighbors = [] # Keep track for the else case\n",
    "\n",
    "        # Use explicit loop \n",
    "        for neigh_node in unvisited_neighbors:\n",
    "            try:\n",
    "                # Safely get node color attribute\n",
    "                node_attrs = graph.nodes.get(neigh_node, {})\n",
    "                node_color = node_attrs.get('color')\n",
    "\n",
    "                if node_color == self.color_preference:\n",
    "                    preferred_unvisited_neighbors.append(neigh_node)\n",
    "                else:\n",
    "                    non_preferred_unvisited_neighbors.append(neigh_node) # Add to non-preferred list\n",
    "\n",
    "            except Exception as e_inner:\n",
    "                # Catch any unexpected errors during attribute access\n",
    "                print(f\"  ERROR accessing node {neigh_node} attributes: {repr(e_inner)}\")\n",
    "                # If error, treat as non-preferred for safety\n",
    "                non_preferred_unvisited_neighbors.append(neigh_node)\n",
    "\n",
    "        # Now proceed with the choice logic\n",
    "        if preferred_unvisited_neighbors:\n",
    "            # Strategy: ALWAYS choose randomly from preferred neighbors if available\n",
    "            return rng.choice(preferred_unvisited_neighbors)\n",
    "        elif non_preferred_unvisited_neighbors: # Check if non-preferred exist before choosing\n",
    "            # Strategy: If no preferred available, choose randomly from any unvisited (non-preferred ones)\n",
    "            return rng.choice(non_preferred_unvisited_neighbors)\n",
    "        else:\n",
    "            # Should only happen if unvisited_neighbors was initially populated but filtering/errors removed all\n",
    "            # Or if graph has isolated nodes not properly handled earlier\n",
    "              print(f\" Warning: Agent at {self.current_node} has no valid unvisited neighbors to move to. Stuck.\")\n",
    "              return None # Truly stuck\n",
    "    # <<< END EXPLICIT LOOP FIX >>>\n",
    "\n",
    "    # traverse_graph calls the fixed find_next_node (no bias factor needed)\n",
    "    def traverse_graph(self, graph: nx.Graph, rng: random.Random) -> Tuple[List[int], int, int]:\n",
    "        \"\"\" Simulates exploration walk using deterministic preference strategy. \"\"\"\n",
    "        steps_taken = 0; self.preferred_nodes_visited = set()\n",
    "        try: # Record start node attributes\n",
    "            start_node_attrs = graph.nodes[self.start_node]; self.visited_shapes.append(start_node_attrs.get('shape', 'unknown')); self.visited_shininess.append(start_node_attrs.get('shiny', 'unknown'))\n",
    "            if start_node_attrs.get('color') == self.color_preference: self.preferred_nodes_visited.add(self.start_node)\n",
    "        except KeyError: self.visited_shapes.append('unknown'); self.visited_shininess.append('unknown')\n",
    "\n",
    "        while len(self.preferred_nodes_visited) < self.target_preferred_count and steps_taken < self.max_steps:\n",
    "            next_node = self.find_next_node(graph, rng) # Calls fixed V5 logic\n",
    "            if next_node is None: break\n",
    "            self.visited_nodes.add(next_node); self.path.append(next_node); self.current_node = next_node; steps_taken += 1\n",
    "            try: # Record visited node attributes\n",
    "                node_attrs = graph.nodes[next_node]; self.visited_shapes.append(node_attrs.get('shape', 'unknown')); self.visited_shininess.append(node_attrs.get('shiny', 'unknown'))\n",
    "                if node_attrs.get('color') == self.color_preference: self.preferred_nodes_visited.add(next_node)\n",
    "            except KeyError: self.visited_shapes.append('unknown'); self.visited_shininess.append('unknown')\n",
    "\n",
    "        preferred_nodes_found_count = len(self.preferred_nodes_visited)\n",
    "        return self.path, preferred_nodes_found_count, steps_taken\n",
    "\n",
    "# ==============================================================================\n",
    "# Graph Generation (Structured Version)\n",
    "# ==============================================================================\n",
    "def generate_structured_colored_graph(num_nodes: int, colors: List[str], edge_probability: float, structure_seed: int, edge_seed: int, noise_level: float = 0.1) -> nx.Graph:\n",
    "    G = nx.Graph(); num_colors = len(colors);\n",
    "    if num_colors == 0: raise ValueError(\"Color list empty.\");\n",
    "    if not 0.0 <= noise_level <= 1.0: raise ValueError(\"Noise level invalid.\")\n",
    "    nodes_per_region = num_nodes // num_colors; region_rng = random.Random(structure_seed); node_assignments = {}\n",
    "    nodes_list = list(range(1, num_nodes + 1)); region_rng.shuffle(nodes_list)\n",
    "    print(f\"  Assigning {num_nodes} nodes to {num_colors} color regions (Noise: {noise_level*100:.1f}%)...\")\n",
    "    for i, node_id in enumerate(nodes_list):\n",
    "        region_index = min(i // nodes_per_region, num_colors - 1); base_color = colors[region_index]; final_color = base_color\n",
    "        if noise_level > 0 and region_rng.random() < noise_level:\n",
    "            possible_noise_colors = [c for c in colors if c != base_color]\n",
    "            if possible_noise_colors: final_color = region_rng.choice(possible_noise_colors)\n",
    "        node_assignments[node_id] = final_color\n",
    "    shape_rng = random.Random(structure_seed + 1); shapes = ['circle', 'square', 'triangle']; shininess_options = ['shiny', 'not_shiny']\n",
    "    for i in range(1, num_nodes + 1):\n",
    "          shape = shape_rng.choice(shapes); shininess = shape_rng.choice(shininess_options)\n",
    "          G.add_node(i, color=node_assignments[i], shape=shape, shiny=shininess)\n",
    "    edge_rng = random.Random(edge_seed); edge_count = 0\n",
    "    for i in range(1, num_nodes + 1):\n",
    "        for j in range(i + 1, num_nodes + 1):\n",
    "            if edge_rng.random() < edge_probability: G.add_edge(i, j); edge_count += 1\n",
    "    if num_nodes > 0 and not nx.is_connected(G): warnings.warn(f\"Generated graph (seed {structure_seed}) not connected.\", stacklevel=2)\n",
    "    return G\n",
    "\n",
    "# ==============================================================================\n",
    "# Simulation (Preference-Driven Goal)\n",
    "# ==============================================================================\n",
    "def simulate_agents(graph: nx.Graph, num_agents: int, num_traversals_per_agent: int, simulation_seed: int, edge_probability: float, target_preferred_count: int, max_steps: int, colors_list: List[str] = ['blue', 'green', 'red', 'yellow']) -> pd.DataFrame:\n",
    "    agent_data = []; node_id_to_color_name = {node: graph.nodes[node].get('color', 'unknown') for node in graph.nodes()}\n",
    "    sim_rng = random.Random(simulation_seed); print(f\" Starting simulations for Seed {simulation_seed}...\")\n",
    "    total_simulations = num_agents * num_traversals_per_agent; completed_simulations = 0; skipped_simulations = 0\n",
    "    print(f\"  NOTE: Agent goal: Find {target_preferred_count} preferred nodes (max {max_steps} steps).\"); print(\"  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\");\n",
    "    nodes_list = list(graph.nodes())\n",
    "    if not nodes_list: print(\" Error: Graph has no nodes.\"); return pd.DataFrame(agent_data)\n",
    "    for i in range(total_simulations):\n",
    "        agent_id = i % num_agents; start_node = sim_rng.choice(nodes_list); color_preference = sim_rng.choice(colors_list)\n",
    "        agent = Agent(start_node, color_preference, target_preferred_count, max_steps)\n",
    "        try: path, preferred_nodes_found, steps_taken = agent.traverse_graph(graph, sim_rng); # Call fixed Agent V5\n",
    "        except Exception as e: print(f\" Error during agent traversal call (Start: {start_node}, Pref: {color_preference}): {repr(e)}\"); skipped_simulations += 1; continue\n",
    "        if not path: print(f\"  Warning: Traversal returned empty path (Start: {start_node}). Skipping.\"); skipped_simulations += 1; continue\n",
    "        mapped_colors = [node_id_to_color_name.get(node, 'unknown') for node in agent.path]; path_len = len(agent.path); preferred_color_path_count = sum(1 for node in agent.path if graph.nodes.get(node, {}).get('color') == agent.color_preference); pref_prop = (preferred_color_path_count / path_len) if path_len > 0 else 0.0\n",
    "        agent_data.append({'Seed': simulation_seed, 'Agent': agent_id, 'Start Node': start_node, 'Color Preference': color_preference, 'Target Preferred Count': target_preferred_count, 'Max Steps': max_steps, 'Actual Steps Taken': steps_taken, 'Preferred Nodes Found': preferred_nodes_found, 'Path Length': path_len, 'Preferred Color Path Count': preferred_color_path_count, 'Preferred_Color_Proportion': pref_prop, 'Path': agent.path, 'Mapped Colors': mapped_colors, 'Visited Shapes': agent.visited_shapes, 'Visited Shininess': agent.visited_shininess, 'Density': edge_probability }); completed_simulations += 1\n",
    "    print(f\"  Finished simulations for Seed {simulation_seed}. Completed: {completed_simulations}, Skipped: {skipped_simulations}\")\n",
    "    return pd.DataFrame(agent_data)\n",
    "\n",
    "# ==============================================================================\n",
    "# Clustering Feature Engineering / Distance Calculation\n",
    "# ==============================================================================\n",
    "def jaccard_distance(set1: Any, set2: Any) -> float:\n",
    "    try: set1 = set(set1) if isinstance(set1, (list, tuple, np.ndarray)) else set(set1) if set1 is not None else set(); set2 = set(set2) if isinstance(set2, (list, tuple, np.ndarray)) else set(set2) if set2 is not None else set()\n",
    "    except TypeError: return 1.0\n",
    "    intersection = len(set1.intersection(set2)); union = len(set1.union(set2));\n",
    "    if union == 0: return 0.0\n",
    "    return 1.0 - intersection / union\n",
    "\n",
    "def extract_boc_features(group: pd.DataFrame) -> Optional[Any]:\n",
    "     if 'Mapped Colors' not in group.columns: print(\"Error:'Mapped Colors'\"); return None\n",
    "     group['Mapped Colors'] = group['Mapped Colors'].apply(lambda x: x if isinstance(x, list) else []) # Ensure list\n",
    "     corpus = group['Mapped Colors'].apply(lambda colors: ' '.join(map(str, colors)))\n",
    "     try: vectorizer = CountVectorizer(); X = vectorizer.fit_transform(corpus); print(f\"  BoC Features extracted with shape: {X.shape}\"); return X\n",
    "     except Exception as e: print(f\"  Error extracting BoC features: {e}\"); return None\n",
    "\n",
    "def extract_combined_features(group: pd.DataFrame) -> Optional[Any]:\n",
    "    print(\"  Extracting Combined Features (BoC + Numerical)...\"); boc_features = extract_boc_features(group);\n",
    "    if boc_features is None: return None\n",
    "    num_cols = ['Path Length', 'Preferred_Color_Proportion'] # Ensure these cols exist\n",
    "    if not all(col in group.columns for col in num_cols): print(f\"Error: Missing {num_cols}\"); return None\n",
    "    try: numerical_features = group[num_cols].values.astype(float)\n",
    "    except Exception as e: print(f\"Error accessing numerical features: {e}\"); return None\n",
    "    try: scaler = StandardScaler(); scaled_numerical = scaler.fit_transform(numerical_features)\n",
    "    except Exception as e: print(f\"Error scaling numerical features: {e}\"); return None\n",
    "    try: combined_features = sparse_hstack((boc_features.tocsr(), scaled_numerical), format='csr'); print(f\"  Combined Features shape: {combined_features.shape}\"); return combined_features\n",
    "    except Exception as e: print(f\"Error combining features: {e}\"); return None\n",
    "\n",
    "# NOTE: Add extract_sequence_features_dl here if using 'sequence_dl' method\n",
    "\n",
    "def calculate_distance_matrix(group: pd.DataFrame, method: str = 'jaccard', **kwargs) -> Tuple[Optional[np.ndarray], Optional[Any], bool, str]:\n",
    "    num_samples = len(group); metric_used = method; distance_matrix = None; feature_matrix = None; is_valid = False\n",
    "    if method == 'jaccard':\n",
    "        metric_used = 'jaccard';\n",
    "        if 'Path' not in group.columns: print(\"Error:'Path'\"); return None, None, False, metric_used\n",
    "        group['Path_Set'] = group['Path'].apply(lambda x: set(x) if isinstance(x, (list, tuple)) else set())\n",
    "        distance_matrix_calc = np.zeros((num_samples, num_samples)); path_sets = group['Path_Set'].tolist()\n",
    "        for i in range(num_samples):\n",
    "            for j in range(i + 1, num_samples):\n",
    "                try: dist = jaccard_distance(path_sets[i], path_sets[j]);\n",
    "                except Exception as e: print(f\"Error Jaccard({i},{j}): {e}\"); return None, None, False, metric_used\n",
    "                if not np.isfinite(dist): raise ValueError(f\"Invalid Jaccard dist: {dist}\")\n",
    "                distance_matrix_calc[i, j] = dist; distance_matrix_calc[j, i] = dist\n",
    "        distance_matrix = distance_matrix_calc; is_valid = True\n",
    "    elif method == 'boc':\n",
    "        metric_used = kwargs.get('metric', 'cosine'); feature_matrix = extract_boc_features(group)\n",
    "        if feature_matrix is not None:\n",
    "            try: condensed_distances = pdist(feature_matrix.toarray(), metric=metric_used); distance_matrix = squareform(condensed_distances); print(f\"  BoC Features + '{metric_used}' distance matrix calculated.\"); is_valid = True\n",
    "            except Exception as e: print(f\"Warn: Could not create distance matrix from BoC: {e}\"); is_valid = False # Fail validation on error\n",
    "        else: return None, None, False, 'boc'\n",
    "    elif method == 'combined':\n",
    "         metric_used = kwargs.get('metric', 'euclidean'); feature_matrix = extract_combined_features(group)\n",
    "         if feature_matrix is not None: is_valid = True; distance_matrix = None; print(f\"  Combined features extracted. Assoc metric: '{metric_used}'\")\n",
    "         else: is_valid = False\n",
    "         return distance_matrix, feature_matrix, is_valid, metric_used # Return features directly\n",
    "    # NOTE: Add elif method == 'sequence_dl': here if using DL features\n",
    "    else: print(f\"Error: Unknown method '{method}'.\"); return None, None, False, 'unknown'\n",
    "    return distance_matrix, feature_matrix, is_valid, metric_used\n",
    "\n",
    "# ==============================================================================\n",
    "# Clustering and Evaluation Function (includes .to_numpy() fix)\n",
    "# ==============================================================================\n",
    "def cluster_and_evaluate(df: pd.DataFrame, n_clusters: int = 4, feature_distance_method: str = 'jaccard', cluster_method: str = 'ward', dist_metric_param: str = 'cosine', linkage_method: str = 'ward' ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Adjust required cols check based on new simulation output if needed\n",
    "    required_cols = ['Seed', 'Color Preference', 'Mapped Colors'] # Base required\n",
    "    if feature_distance_method == 'jaccard': required_cols.append('Path')\n",
    "    elif feature_distance_method == 'combined': required_cols.extend(['Path Length', 'Preferred Color Path Count', 'Path', 'Mapped Colors'])\n",
    "    elif feature_distance_method == 'boc': required_cols.extend(['Path', 'Mapped Colors'])\n",
    "    # NOTE: Add elif for 'sequence_dl' if using it\n",
    "\n",
    "    if not all(col in df.columns for col in required_cols): raise ValueError(f\"Missing columns for eval: {required_cols}. Available: {df.columns.tolist()}\")\n",
    "    print(f\"\\n--- Starting Clustering Evaluation ---\"); print(f\" Feature/Distance Method: {feature_distance_method}\"); print(f\" Clustering Algorithm: {cluster_method}\"); print(f\" Target Clusters (k): {n_clusters}\")\n",
    "    results_summary = []; processed_groups = []\n",
    "    for seed, group in df.groupby('Seed'):\n",
    "        print(f\" Processing Seed {seed}...\"); group = group.copy(); num_samples = len(group); group['Cluster'] = -1\n",
    "        if num_samples < 2: print(f\"  Skipping Seed {seed}: Samples < 2.\"); processed_groups.append(group); continue\n",
    "\n",
    "        # --- Ensure calculate_distance_matrix handles the selected method ---\n",
    "        # It should return feature_matrix if needed by the clustering method\n",
    "        distance_matrix, feature_matrix, is_valid, eval_metric = calculate_distance_matrix(\n",
    "            group,\n",
    "            method=feature_distance_method,\n",
    "            metric=dist_metric_param # Passed as kwarg for BoC/Combined metrics\n",
    "            # Add kwargs for DL params if using sequence_dl method\n",
    "        )\n",
    "\n",
    "        if not is_valid:\n",
    "            print(f\"  Skipping Seed {seed}: Feature/Distance calculation failed or invalid.\")\n",
    "            processed_groups.append(group); continue\n",
    "\n",
    "        clusters = None; actual_n_clusters = min(n_clusters, num_samples) if num_samples > 0 else 1;\n",
    "        if actual_n_clusters < 1: actual_n_clusters = 1\n",
    "\n",
    "        try: # Clustering Block\n",
    "            if cluster_method in ['ward', 'complete', 'average', 'single']: # Hierarchical\n",
    "                 linkage_input_dm = distance_matrix\n",
    "                 if linkage_input_dm is None: # Need distances from features\n",
    "                     if feature_matrix is not None:\n",
    "                         print(f\"  Calculating '{dist_metric_param}' distance matrix for hierarchical on features...\")\n",
    "                         try:\n",
    "                            feature_array_for_pdist = feature_matrix.toarray() if hasattr(feature_matrix, \"toarray\") else feature_matrix\n",
    "                            if not np.isfinite(feature_array_for_pdist).all():\n",
    "                                warnings.warn(f\"NaN/Inf found in features for pdist (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "                                feature_array_for_pdist = np.nan_to_num(feature_array_for_pdist, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                            linkage_input_dm = squareform(pdist(feature_array_for_pdist, metric=dist_metric_param))\n",
    "                         except Exception as e_pdist: raise ValueError(f\"Failed distance calc for hierarchical: {e_pdist}\") from e_pdist\n",
    "                     else: raise ValueError(\"Hierarchical requires distances, but neither distance matrix nor features available.\")\n",
    "                 # Validate distance matrix for linkage\n",
    "                 if not isinstance(linkage_input_dm, np.ndarray) or linkage_input_dm.ndim!=2 or linkage_input_dm.shape[0]!=linkage_input_dm.shape[1]: raise ValueError(\"Invalid DM shape.\")\n",
    "                 if np.isnan(linkage_input_dm).any() or np.isinf(linkage_input_dm).any(): raise ValueError(\"NaN/Inf in DM.\")\n",
    "                 if not np.allclose(linkage_input_dm, linkage_input_dm.T): warnings.warn(f\"DM not symmetric (Seed {seed}). Forcing symmetry.\", stacklevel=2); linkage_input_dm = (linkage_input_dm + linkage_input_dm.T) / 2\n",
    "                 np.fill_diagonal(linkage_input_dm, 0)\n",
    "                 linkage_input_dm[linkage_input_dm < 0] = 0 # Ensure non-negative\n",
    "\n",
    "                 try: condensed_distance = squareform(linkage_input_dm, checks=True)\n",
    "                 except ValueError as sq_err: raise ValueError(f\"Squareform check failed: {sq_err}\") from sq_err\n",
    "                 if condensed_distance is None or not np.isfinite(condensed_distance).all(): raise ValueError(\"Invalid condensed distance array.\")\n",
    "                 Z = linkage(condensed_distance, method=cluster_method);\n",
    "                 if Z is None or not isinstance(Z, np.ndarray): raise TypeError(\"Linkage failed.\")\n",
    "                 clusters = fcluster(Z, t=actual_n_clusters, criterion='maxclust')\n",
    "                 if clusters is None or not isinstance(clusters, np.ndarray): raise TypeError(\"fcluster failed.\")\n",
    "\n",
    "            elif cluster_method in ['kmeans', 'gmm']: # Feature-based\n",
    "                 current_feature_matrix = feature_matrix # Assume calculate_distance_matrix returned features if needed\n",
    "                 if current_feature_matrix is None:\n",
    "                      # This path might indicate an issue if features were expected but not returned\n",
    "                      raise ValueError(f\"Features required for {cluster_method} but not available for method {feature_distance_method}.\")\n",
    "\n",
    "                 # Ensure dense array for KMeans/GMM\n",
    "                 feature_array_cluster = current_feature_matrix.toarray() if hasattr(current_feature_matrix, \"toarray\") else current_feature_matrix\n",
    "\n",
    "                 if feature_array_cluster.shape[0] < actual_n_clusters:\n",
    "                      warnings.warn(f\"Samples ({feature_array_cluster.shape[0]}) < clusters/components ({actual_n_clusters}) for {cluster_method} (Seed {seed}). Reducing target.\", stacklevel=2)\n",
    "                      actual_n_clusters = max(1, feature_array_cluster.shape[0])\n",
    "\n",
    "                 if cluster_method == 'kmeans':\n",
    "                     if not np.isfinite(feature_array_cluster).all():\n",
    "                         warnings.warn(f\"NaN/Inf found in features for KMeans (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "                         feature_array_cluster = np.nan_to_num(feature_array_cluster, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                     kmeans = KMeans(n_clusters=actual_n_clusters, random_state=seed, n_init=10, verbose=0);\n",
    "                     clusters = kmeans.fit_predict(feature_array_cluster); clusters += 1 # Make 1-based\n",
    "                 elif cluster_method == 'gmm':\n",
    "                     if not np.isfinite(feature_array_cluster).all():\n",
    "                         warnings.warn(f\"NaN/Inf found in features for GMM (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "                         feature_array_cluster = np.nan_to_num(feature_array_cluster, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                     gmm = GaussianMixture(n_components=actual_n_clusters, random_state=seed, verbose=0, n_init=5);\n",
    "                     clusters = gmm.fit_predict(feature_array_cluster); clusters += 1 # Make 1-based\n",
    "            else: raise ValueError(f\"Unsupported cluster_method: {cluster_method}\")\n",
    "\n",
    "            if clusters is None or len(clusters) != num_samples:\n",
    "                 raise ValueError(f\"Clustering failed or produced incorrect number of labels ({len(clusters)} vs {num_samples} samples).\")\n",
    "\n",
    "            group['Cluster'] = clusters; num_unique_clusters_found = len(set(c for c in clusters if c != -1))\n",
    "        except Exception as e: print(f\"  Error during clustering for Seed {seed}: {repr(e)}\"); import traceback; traceback.print_exc(); processed_groups.append(group); continue\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        silhouette_avg = np.nan; ari_score = np.nan; nmi_score = np.nan\n",
    "        valid_cluster_mask = group['Cluster'] != -1; clusters_for_eval = group.loc[valid_cluster_mask, 'Cluster']\n",
    "        num_valid_samples_for_eval = len(clusters_for_eval); num_clusters_for_eval = len(set(clusters_for_eval))\n",
    "\n",
    "        if num_clusters_for_eval > 1 and num_clusters_for_eval < num_valid_samples_for_eval: # Silhouette Calc\n",
    "            silhouette_input_data = None; silhouette_metric = eval_metric # Use the metric associated with the features/distances\n",
    "            can_use_precomputed = (distance_matrix is not None)\n",
    "\n",
    "            if can_use_precomputed and (feature_distance_method == 'jaccard' or cluster_method in ['ward', 'complete', 'average', 'single']):\n",
    "                 # Use precomputed distance matrix if appropriate (Jaccard or Hierarchical on distances)\n",
    "                 silhouette_metric = 'precomputed';\n",
    "                 valid_mask_np = valid_cluster_mask.to_numpy() # Use numpy mask\n",
    "                 if len(valid_mask_np) == distance_matrix.shape[0]:\n",
    "                     valid_distance_matrix = distance_matrix[np.ix_(valid_mask_np, valid_mask_np)]\n",
    "                     if valid_distance_matrix.shape[0] > 1:\n",
    "                         try: silhouette_avg = silhouette_score(valid_distance_matrix, clusters_for_eval, metric='precomputed')\n",
    "                         except Exception as e_sil: print(f\"   Warning: Silhouette (precomputed) error: {repr(e_sil)}\")\n",
    "                 else: print(f\"   Warning: Index mismatch Silhouette submatrix (Seed {seed}).\")\n",
    "\n",
    "            elif feature_matrix is not None: # Try features if available (BoC, Combined, DL)\n",
    "                 current_feature_matrix_sil = feature_matrix # Ensure we use the feature matrix calculated earlier\n",
    "                 if current_feature_matrix_sil.shape[0] == num_samples:\n",
    "\n",
    "                     # ============================================= #\n",
    "                     # <<< FIX APPLIED HERE >>>\n",
    "                     # Convert boolean Series mask to NumPy array before indexing features\n",
    "                     valid_feature_matrix_sil = current_feature_matrix_sil[valid_cluster_mask.to_numpy()]\n",
    "                     # ============================================= #\n",
    "\n",
    "                     if valid_feature_matrix_sil.shape[0] > 1:\n",
    "                         try:\n",
    "                            # Ensure features are dense for silhouette score\n",
    "                            feature_array_sil = valid_feature_matrix_sil.toarray() if hasattr(valid_feature_matrix_sil, \"toarray\") else valid_feature_matrix_sil\n",
    "                            # Handle potential NaN/Inf before Silhouette\n",
    "                            if not np.isfinite(feature_array_sil).all():\n",
    "                                warnings.warn(f\"NaN/Inf found in features for Silhouette (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "                                feature_array_sil = np.nan_to_num(feature_array_sil, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                            silhouette_avg = silhouette_score(feature_array_sil, clusters_for_eval, metric=silhouette_metric) # Use eval_metric\n",
    "                         except Exception as e_sil: print(f\"   Warning: Silhouette (feature, metric='{silhouette_metric}') error: {repr(e_sil)}\")\n",
    "                 else: print(f\"   Warning: Feature matrix shape mismatch Silhouette (Seed {seed}).\")\n",
    "\n",
    "            if silhouette_avg is np.nan and num_clusters_for_eval > 1:\n",
    "                 print(f\"   Skipping Silhouette: Input data invalid or unavailable (Seed {seed})\")\n",
    "\n",
    "        if num_valid_samples_for_eval > 0: # ARI/NMI Calc\n",
    "             try:\n",
    "                true_labels_for_eval = group.loc[valid_cluster_mask, 'Color Preference'];\n",
    "                ari_score = adjusted_rand_score(true_labels_for_eval, clusters_for_eval);\n",
    "                nmi_score = normalized_mutual_info_score(true_labels_for_eval, clusters_for_eval)\n",
    "             except Exception as e_gnd: print(f\"   Warning: Could not compute ARI/NMI Seed {seed}: {repr(e_gnd)}\")\n",
    "\n",
    "        formatted_cluster_color_percentages = {}; max_color_separation = np.nan\n",
    "        try: # Purity Calc\n",
    "             valid_group_for_purity = group[group['Cluster'] != -1]\n",
    "             if not valid_group_for_purity.empty:\n",
    "                 if 'Mapped Colors' in valid_group_for_purity.columns:\n",
    "                     exploded_group = valid_group_for_purity.explode('Mapped Colors').dropna(subset=['Mapped Colors']); exploded_group.rename(columns={'Mapped Colors': 'Flat Colors'}, inplace=True)\n",
    "                     color_counts = exploded_group.groupby(['Cluster', 'Flat Colors']).size().unstack(fill_value=0)\n",
    "                     all_possible_colors = sorted(df['Color Preference'].dropna().unique())\n",
    "                     for color in all_possible_colors:\n",
    "                         if color not in color_counts.columns: color_counts[color] = 0\n",
    "                     color_counts = color_counts[all_possible_colors]; cluster_sums = color_counts.sum(axis=1); safe_sums = cluster_sums.replace(0, 1)\n",
    "                     color_percentages = color_counts.div(safe_sums, axis=0).mul(100); cluster_color_percentages_dict = color_percentages.round(2).apply(lambda r: r.dropna().to_dict(), axis=1).to_dict()\n",
    "                     formatted_cluster_color_percentages = {int(k): {c: f\"{p:.2f}%\" for c, p in v.items()} for k, v in cluster_color_percentages_dict.items()}\n",
    "                     if not color_percentages.empty: max_color_separation = color_percentages.apply(lambda r: r.max() - r.min() if not r.empty else 0.0, axis=1).max()\n",
    "                     else: max_color_separation = 0.0\n",
    "                 else: print(f\"   Warning: 'Mapped Colors' column missing for purity calc (Seed {seed}).\")\n",
    "        except Exception as e: print(f\"   Warning: Color percentage error Seed {seed}: {e}\")\n",
    "\n",
    "        results_summary.append({'Seed': seed, 'Algorithm': cluster_method, 'Feature_Metric': f\"{feature_distance_method}({eval_metric})\", 'Num_Clusters_Target': n_clusters, 'Num_Clusters_Found': num_clusters_for_eval, 'Cluster_Color_Percentage': formatted_cluster_color_percentages, 'Silhouette': silhouette_avg, 'ARI': ari_score, 'NMI': nmi_score, 'Max_Color_Separation': max_color_separation })\n",
    "        processed_groups.append(group) # Add group with clusters\n",
    "\n",
    "    if not processed_groups: warnings.warn(\"No groups processed.\"); return pd.DataFrame(results_summary), df.copy()\n",
    "    df_with_clusters = pd.concat(processed_groups).reset_index(drop=True); results_summary_df = pd.DataFrame(results_summary)\n",
    "    print(\"\\nClustering and evaluation complete.\")\n",
    "    return results_summary_df, df_with_clusters\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Plotting Functions\n",
    "# ==============================================================================\n",
    "def plot_colored_graph(G: nx.Graph, seed: int, output_dir: str = \"plots\"):\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    filepath = os.path.join(output_dir, f\"graph_seed_{seed}.png\")\n",
    "    plt.figure(figsize=(12, 10)); pos = nx.spring_layout(G, seed=42); node_colors = [G.nodes[node].get('color', 'gray') for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=200, alpha=0.9); nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
    "    plt.title(f\"Graph Structure (Seed {seed})\", fontsize=16); plt.axis('off'); plt.tight_layout(); plt.savefig(filepath); print(f\" Saved graph plot to {filepath}\"); plt.close()\n",
    "\n",
    "def plot_color_confusion_matrix(df: pd.DataFrame, cluster_col: str = 'Cluster', true_color_col: str = 'Color Preference', figsize: Tuple[int, int] = (8, 7), cmap: str = 'Blues', output_dir: str = \"plots\"):\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    plot_filename = f\"confusion_matrix_{cluster_col}_vs_{true_color_col}.png\"; filepath = os.path.join(output_dir, plot_filename)\n",
    "    if cluster_col not in df.columns: raise ValueError(f\"Cluster column '{cluster_col}' not found.\")\n",
    "    if true_color_col not in df.columns: raise ValueError(f\"True color column '{true_color_col}' not found.\")\n",
    "\n",
    "    # Ensure cluster column is numeric and filter invalid/noise clusters (e.g., -1)\n",
    "    df[cluster_col] = pd.to_numeric(df[cluster_col], errors='coerce')\n",
    "    df_valid = df.dropna(subset=[cluster_col, true_color_col]).copy() # Drop NaNs in both relevant columns\n",
    "    df_valid[cluster_col] = df_valid[cluster_col].astype(int);\n",
    "    df_valid = df_valid[df_valid[cluster_col] > 0] # Assuming clusters are 1-based and >0 are valid\n",
    "\n",
    "    if df_valid.empty: print(f\"Warning: No valid data for confusion matrix (Cluster Col: {cluster_col}).\"); return\n",
    "\n",
    "    unique_colors = sorted(df_valid[true_color_col].unique());\n",
    "    if not unique_colors: print(f\"Error: No valid unique values in '{true_color_col}'.\"); return\n",
    "\n",
    "    unique_pred_labels = sorted(df_valid[cluster_col].unique())\n",
    "    if not unique_pred_labels: print(f\"Error: No valid predicted cluster labels found in '{cluster_col}'.\"); return\n",
    "\n",
    "    # Map true colors to integers for rows\n",
    "    color_to_int_mapping = {color: i + 1 for i, color in enumerate(unique_colors)};\n",
    "    # Create labels based on the mapping\n",
    "    matrix_row_labels_int = list(color_to_int_mapping.values())\n",
    "    matrix_row_labels_names = [color for color, i in sorted(color_to_int_mapping.items(), key=lambda item: item[1])] # Ensure order matches ints\n",
    "\n",
    "    # Use actual predicted labels for columns\n",
    "    matrix_col_labels_int = unique_pred_labels\n",
    "    matrix_col_labels_names = [f\"Cluster {i}\" for i in matrix_col_labels_int]\n",
    "\n",
    "    print(f\"\\n--- Confusion Matrix Generation ---\");\n",
    "    print(f\" Row Mapping (True Color -> Int): {color_to_int_mapping}\")\n",
    "    print(f\" Col Labels (Predicted Clusters): {matrix_col_labels_int}\")\n",
    "\n",
    "    y_true = df_valid[true_color_col].map(color_to_int_mapping).astype(int);\n",
    "    y_pred = df_valid[cluster_col] # Already filtered and int\n",
    "\n",
    "    # Calculate matrix using consistent labels covering observed range\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=matrix_row_labels_int)\n",
    "\n",
    "    # Adjust columns if predicted labels don't align perfectly with 1..N\n",
    "    # This part assumes rows correspond to 1..num_true_colors, cols might be sparse\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, index=matrix_row_labels_names) # Rows match true colors\n",
    "    # If y_pred doesn't contain all labels from 1..k, conf_matrix might have fewer columns than expected.\n",
    "    # We label columns based on the actual unique predicted labels found.\n",
    "    if conf_matrix.shape[1] == len(matrix_col_labels_names):\n",
    "         conf_matrix_df.columns = matrix_col_labels_names\n",
    "    else:\n",
    "         # Handle potential mismatch if confusion_matrix didn't use predicted labels correctly\n",
    "         # This might happen if labels argument forces shape; safer to label based on what was found\n",
    "         warnings.warn(f\"Confusion matrix column count ({conf_matrix.shape[1]}) mismatch with unique predicted labels ({len(matrix_col_labels_names)}). Labeling sequentially.\", stacklevel=2)\n",
    "         conf_matrix_df.columns = [f\"Pred {i}\" for i in range(conf_matrix.shape[1])]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=figsize); sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap=cmap, cbar=True, linewidths=.5, linecolor='lightgray', annot_kws={\"size\": 10})\n",
    "    title = f'Confusion Matrix: Predicted Cluster ({cluster_col}) vs True Color ({true_color_col})'\n",
    "    if 'Remapped' in cluster_col: title += \" (Remapped)\"\n",
    "    plt.title(title, fontsize=14); plt.ylabel(f'True Color (`{true_color_col}`)', fontsize=12); plt.xlabel(f'Predicted Label (`{cluster_col}`)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10); plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.tight_layout(); plt.savefig(filepath); print(f\" Saved confusion matrix plot to {filepath}\"); plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block -> STRUCTURED GRAPH + BOC/KMEANS\n",
    "# Includes Cluster Label Remapping for Plotting\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Parameters ---\n",
    "    NUM_NODES = 40; COLORS = ['blue', 'green', 'red', 'yellow', 'white']; EDGE_PROBABILITY = 0.20\n",
    "    NUM_SEEDS = 10; NUM_AGENTS = 5; NUM_TRAVERSALS_PER_AGENT = 50;\n",
    "\n",
    "    # --- Simulation Parameters ---\n",
    "    TARGET_PREFERRED_COUNT = 5 # Agent goal\n",
    "    MAX_STEPS = 50             # Step limit\n",
    "    GRAPH_NOISE_LEVEL = 0.0 # <<< NO NOISE in structured graph >>>\n",
    "\n",
    "    # --- Clustering Set to BOC + KMEANS ---\n",
    "    N_CLUSTERS = len(COLORS) # Should be len(COLORS)\n",
    "    FEATURE_DISTANCE_METHOD = 'boc' # Options: 'jaccard', 'boc', 'combined', 'sequence_dl'\n",
    "    CLUSTER_METHOD = 'kmeans'       # Options: 'kmeans', 'gmm', 'ward', 'complete', etc.\n",
    "    DIST_METRIC = 'cosine'          # Metric for BoC distance or Silhouette on features ('euclidean', 'cosine', etc.)\n",
    "\n",
    "    # --- Output Directory ---\n",
    "    OUTPUT_DIR = f\"clustering_results_{FEATURE_DISTANCE_METHOD}_{CLUSTER_METHOD}_k{N_CLUSTERS}_agent_v5_structured_NO_NOISE\"\n",
    "    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # --- Run Simulation ---\n",
    "    print(f\"--- Starting Simulation Phase (Agent V5 - Deterministic Preference, Structured Graph NO NOISE) ---\")\n",
    "    all_results_list = []; start_time_sim = time.time()\n",
    "    for seed in range(NUM_SEEDS):\n",
    "        graph_attribute_seed = seed; graph_edge_seed = seed + NUM_SEEDS\n",
    "        print(f\"\\nGenerating structured graph for Seed {seed}...\")\n",
    "        G = generate_structured_colored_graph(NUM_NODES, COLORS, EDGE_PROBABILITY,\n",
    "                                              graph_attribute_seed, graph_edge_seed,\n",
    "                                              noise_level=GRAPH_NOISE_LEVEL)\n",
    "        print(f\"Running Simulation for Seed {seed}...\")\n",
    "        simulation_seed = seed\n",
    "        df_agents = simulate_agents(G, NUM_AGENTS, NUM_TRAVERSALS_PER_AGENT,\n",
    "                                    simulation_seed, EDGE_PROBABILITY,\n",
    "                                    target_preferred_count=TARGET_PREFERRED_COUNT,\n",
    "                                    max_steps=MAX_STEPS,\n",
    "                                    colors_list=COLORS)\n",
    "        all_results_list.append(df_agents)\n",
    "    all_simulation_data = pd.concat(all_results_list, ignore_index=True); end_time_sim = time.time()\n",
    "    print(f\"\\n--- Simulation Phase Complete ---\"); print(f\"Generated {len(all_simulation_data)} records in {end_time_sim - start_time_sim:.2f}s.\")\n",
    "\n",
    "    # Check if simulation produced data\n",
    "    if all_simulation_data.empty:\n",
    "         print(\"\\nERROR: Simulation produced no data. Exiting.\")\n",
    "    else:\n",
    "        # --- Run Clustering and Evaluation ---\n",
    "        print(f\"\\n--- Starting Clustering Phase ---\"); start_time_cluster = time.time()\n",
    "        summary_results, data_with_clusters = cluster_and_evaluate(\n",
    "            all_simulation_data.copy(),\n",
    "            n_clusters=N_CLUSTERS,\n",
    "            feature_distance_method=FEATURE_DISTANCE_METHOD,\n",
    "            cluster_method=CLUSTER_METHOD,\n",
    "            dist_metric_param=DIST_METRIC,\n",
    "            # Pass DL params here if using sequence_dl:\n",
    "            # color_vocab=COLORS, max_seq_len=MAX_SEQ_LEN, embedding_dim=EMBEDDING_DIM, lstm_units=LSTM_UNITS\n",
    "        )\n",
    "        end_time_cluster = time.time(); print(f\"\\n--- Clustering Phase Complete ---\"); print(f\"Completed in {end_time_cluster - start_time_cluster:.2f}s.\")\n",
    "\n",
    "        # --- Display and Save Results ---\n",
    "        print(\"\\n--- Clustering Summary Results ---\"); pd.set_option('display.max_rows', 50); pd.set_option('display.max_columns', None); pd.set_option('display.width', 120); pd.set_option('display.max_colwidth', 150)\n",
    "        if not summary_results.empty:\n",
    "            print(summary_results.round(4).to_string()); avg_ari = summary_results['ARI'].mean(); avg_nmi = summary_results['NMI'].mean(); avg_silhouette = summary_results['Silhouette'].mean()\n",
    "            print(\"-\" * 50); print(f\"Avg ARI: {avg_ari:.4f}\"); print(f\"Avg NMI: {avg_nmi:.4f}\"); print(f\"Avg Silhouette: {avg_silhouette:.4f}\"); print(\"-\" * 50)\n",
    "            summary_filename = f\"summary_{FEATURE_DISTANCE_METHOD}_{CLUSTER_METHOD}_k{N_CLUSTERS}_agent_v5_structured_NO_NOISE.csv\"\n",
    "            summary_filepath = os.path.join(OUTPUT_DIR, summary_filename)\n",
    "            try: summary_results.to_csv(summary_filepath, index=False); print(f\"Summary results saved to {summary_filepath}\")\n",
    "            except Exception as e: print(f\"Error saving summary: {e}\")\n",
    "        else: print(\"No summary results generated.\")\n",
    "\n",
    "        # --- Generate and Save Confusion Matrix (with Optimal Remapping) ---\n",
    "        if not data_with_clusters.empty and 'Cluster' in data_with_clusters.columns and data_with_clusters['Cluster'].notna().any() and data_with_clusters['Cluster'][data_with_clusters['Cluster'] > 0].max() > 0 :\n",
    "             print(\"\\n--- Remapping cluster labels for optimal visualization using Hungarian algorithm ---\")\n",
    "             plot_original = False # Flag to plot original if remapping fails\n",
    "             try:\n",
    "                 # Ensure cluster column is int, handle potential floats/NaNs introduced earlier\n",
    "                 data_with_clusters['Cluster'] = pd.to_numeric(data_with_clusters['Cluster'], errors='coerce')\n",
    "                 valid_clusters_df = data_with_clusters.dropna(subset=['Cluster', 'Color Preference']).copy()\n",
    "                 valid_clusters_df['Cluster'] = valid_clusters_df['Cluster'].astype(int)\n",
    "                 valid_clusters_df = valid_clusters_df[valid_clusters_df['Cluster'] > 0].copy()\n",
    "\n",
    "                 if valid_clusters_df.empty: raise ValueError(\"No valid clusters (>0) found after filtering for remapping.\")\n",
    "\n",
    "                 unique_colors = sorted(valid_clusters_df['Color Preference'].unique())\n",
    "                 true_color_to_plot_int = {color: i + 1 for i, color in enumerate(unique_colors)}\n",
    "                 plot_int_to_true_color = {v: k for k,v in true_color_to_plot_int.items()}\n",
    "                 plot_labels_ordered = [plot_int_to_true_color.get(i, f'Unknown {i}') for i in range(1, len(unique_colors) + 1)]\n",
    "                 print(f\"  Target Plot Order Mapping (Rows): {true_color_to_plot_int}\")\n",
    "\n",
    "                 y_true_numeric_target_order = valid_clusters_df['Color Preference'].map(true_color_to_plot_int).astype(int)\n",
    "                 y_pred_original_cluster_labels = valid_clusters_df['Cluster'] # Original Cluster labels (1-based, filtered > 0)\n",
    "\n",
    "                 # Labels for confusion_matrix calculation\n",
    "                 true_labels_for_cm = list(range(1, len(unique_colors) + 1)) # Corresponds to rows\n",
    "                 pred_labels_for_cm = sorted(y_pred_original_cluster_labels.unique()) # Corresponds to columns found\n",
    "                 print(f\"  Predicted Labels Found for CM Cols: {pred_labels_for_cm}\")\n",
    "\n",
    "                 raw_cm = confusion_matrix(y_true_numeric_target_order,\n",
    "                                           y_pred_original_cluster_labels,\n",
    "                                           labels=true_labels_for_cm) # Rows match true labels 1..N\n",
    "\n",
    "                 # Pad columns if necessary (if some clusters 1..k were not predicted)\n",
    "                 if raw_cm.shape[1] < len(pred_labels_for_cm):\n",
    "                     # This case indicates an issue - confusion_matrix rows/cols depend on labels present in y_true/y_pred\n",
    "                     # Recalculate ensuring prediction labels are also considered if needed, or adjust cost matrix logic\n",
    "                      print(\"  Recalculating CM with explicit pred labels...\")\n",
    "                      all_labels = sorted(list(set(true_labels_for_cm) | set(pred_labels_for_cm)))\n",
    "                      raw_cm = confusion_matrix(y_true_numeric_target_order,\n",
    "                                                y_pred_original_cluster_labels,\n",
    "                                                labels=all_labels) # Use all labels? Check documentation\n",
    "                      # For assignment, we need cost matrix based on true vs predicted\n",
    "                      # Let's stick to the original raw_cm and pad the cost matrix\n",
    "\n",
    "                 print(\"  Raw Confusion Matrix (Rows: True Target Int, Cols: Based on True Target Int Range):\")\n",
    "                 print(raw_cm)\n",
    "\n",
    "                 cost_matrix = -raw_cm\n",
    "                 n_rows, n_cols = cost_matrix.shape # rows=true_labels, cols should match based on labels=true_labels_for_cm\n",
    "\n",
    "                 # We need to map *predicted* cluster labels (columns derived from raw_cm) to *true* target ints (rows)\n",
    "                 # The Hungarian algorithm works on a cost matrix where rows/cols represent items to be matched.\n",
    "                 # Let rows be true labels (1..k_true), cols be predicted labels (found values)\n",
    "                 # Need a cost matrix: rows=true_labels, cols=predicted_labels\n",
    "                 # We can build this by iterating or using crosstab\n",
    "\n",
    "                 print(\"  Building Cost Matrix (True Rows vs Predicted Cols)...\")\n",
    "                 ct = pd.crosstab(y_true_numeric_target_order, y_pred_original_cluster_labels)\n",
    "                 # Reindex to ensure all true labels (rows) and predicted labels (cols) are present\n",
    "                 ct = ct.reindex(index=true_labels_for_cm, columns=pred_labels_for_cm, fill_value=0)\n",
    "                 cost_matrix = -ct.values # Maximize overlap = minimize negative count\n",
    "                 print(\"  Cost Matrix Shape:\", cost_matrix.shape)\n",
    "\n",
    "\n",
    "                 # Ensure cost matrix is square for assignment (or handle rectangular)\n",
    "                 n_rows, n_cols = cost_matrix.shape\n",
    "                 if n_rows != n_cols:\n",
    "                     warnings.warn(f\"Cost matrix ({n_rows}x{n_cols}) not square. Assignment might be partial.\", stacklevel=2)\n",
    "                     # Rectangular assignment is handled by linear_sum_assignment\n",
    "\n",
    "                 row_ind, col_ind = linear_sum_assignment(cost_matrix) # row_ind maps to true labels, col_ind maps to predicted labels\n",
    "\n",
    "                 remapping_dict = {}\n",
    "                 print(\"  Optimal Assignment (True Target Int -> Original Predicted Cluster Label):\")\n",
    "                 for r, c in zip(row_ind, col_ind):\n",
    "                     # Check bounds based on the cost matrix dimensions\n",
    "                     if r < n_rows and c < n_cols:\n",
    "                         target_plot_int = true_labels_for_cm[r] # The true label integer (row)\n",
    "                         original_cluster_label = pred_labels_for_cm[c] # The predicted label integer (column)\n",
    "                         true_color_name = plot_int_to_true_color.get(target_plot_int, f\"? {target_plot_int}\")\n",
    "                         print(f\"    True '{true_color_name}' (Target Int {target_plot_int}) best matches Original Cluster Label {original_cluster_label}\")\n",
    "                         remapping_dict[original_cluster_label] = target_plot_int\n",
    "                     else: print(f\"    Warning: Skipping assignment row={r}, col={c} - index out of bounds.\")\n",
    "\n",
    "\n",
    "                 print(f\"  Cluster Remapping Dict (Original Cluster -> Target Plot Int): {remapping_dict}\")\n",
    "                 # Apply mapping, assign -1 to clusters that couldn't be mapped (e.g., if n_pred < n_true)\n",
    "                 data_with_clusters['Cluster_Remapped'] = data_with_clusters['Cluster'].map(remapping_dict).fillna(-1).astype(int)\n",
    "\n",
    "                 print(\"\\n--- Generating Confusion Matrix (using OPTIMALLY REMAPPED cluster labels) ---\")\n",
    "                 plot_output_dir = os.path.join(OUTPUT_DIR, \"plots_remapped_optimal\")\n",
    "                 # Use the remapped column for plotting\n",
    "                 plot_color_confusion_matrix(data_with_clusters, cluster_col='Cluster_Remapped', true_color_col='Color Preference', output_dir=plot_output_dir)\n",
    "\n",
    "                 # Rename the output file for clarity\n",
    "                 original_plot_name = f\"confusion_matrix_Cluster_Remapped_vs_Color Preference.png\"\n",
    "                 new_plot_name = f\"confusion_matrix_OPTIMAL_REORDERED_{FEATURE_DISTANCE_METHOD}_{CLUSTER_METHOD}_k{N_CLUSTERS}.png\"\n",
    "                 try:\n",
    "                     if not os.path.exists(plot_output_dir): os.makedirs(plot_output_dir)\n",
    "                     source_path = os.path.join(plot_output_dir, original_plot_name)\n",
    "                     dest_path = os.path.join(plot_output_dir, new_plot_name)\n",
    "                     if os.path.exists(source_path):\n",
    "                          os.rename(source_path, dest_path)\n",
    "                          print(f\" Renamed plot to {new_plot_name}\")\n",
    "                     else: print(f\" Plot file not found for renaming: {original_plot_name}\")\n",
    "                 except Exception as e_mv: print(f\" Could not rename remapped plot: {e_mv}\")\n",
    "\n",
    "             except ImportError: print(\"\\n--- Cluster Remapping Failed: scipy.optimize not found. ---\"); plot_original = True\n",
    "             except Exception as e:\n",
    "                  print(f\"\\n--- Could not remap labels or generate remapped plot: {repr(e)} ---\");\n",
    "                  import traceback\n",
    "                  traceback.print_exc()\n",
    "                  plot_original = True\n",
    "\n",
    "             # Fallback to plotting original if remapping failed or flag set\n",
    "             if plot_original:\n",
    "                 print(\" Plotting confusion matrix with original cluster labels due to remapping failure or issue.\")\n",
    "                 plot_output_dir = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "                 plot_color_confusion_matrix(data_with_clusters, cluster_col='Cluster', true_color_col='Color Preference', output_dir=plot_output_dir)\n",
    "\n",
    "        else: print(\"\\nSkipping confusion matrix plot (No valid clusters found or data empty).\")\n",
    "\n",
    "    print(\"\\nPipeline finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88409ce5-599e-48e5-aeb2-21ceccc7026e",
   "metadata": {},
   "source": [
    "Critique of simulated data, Justification & Validity of the Simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8affffb-ee46-4de4-9128-9ac2d720c453",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Sensitivity Analysis ---\n",
      "Testing 24 parameter combinations.\n",
      "Using 3 simulation seeds per combination.\n",
      "Fixed Clustering: boc / kmeans / k=5\n",
      "Results will be saved in: sensitivity_analysis_20250415_164826\n",
      "\n",
      "[1/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40} -> ARI: 0.949, NMI: 0.947, Sil: 0.690\n",
      "  Finished testing params in 0.16s\n",
      "\n",
      "[2/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60} -> ARI: 0.949, NMI: 0.947, Sil: 0.690\n",
      "  Finished testing params in 0.16s\n",
      "\n",
      "[3/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40} -> ARI: 1.000, NMI: 1.000, Sil: 0.771\n",
      "  Finished testing params in 0.18s\n",
      "\n",
      "[4/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60} -> ARI: 1.000, NMI: 1.000, Sil: 0.771\n",
      "  Finished testing params in 0.19s\n",
      "\n",
      "[5/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40} -> ARI: 0.994, NMI: 0.994, Sil: 0.818\n",
      "  Finished testing params in 0.15s\n",
      "\n",
      "[6/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60} -> ARI: 0.994, NMI: 0.994, Sil: 0.818\n",
      "  Finished testing params in 0.17s\n",
      "\n",
      "[7/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40} -> ARI: 1.000, NMI: 1.000, Sil: 0.904\n",
      "  Finished testing params in 0.17s\n",
      "\n",
      "[8/24] Testing Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.0, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60} -> ARI: 1.000, NMI: 1.000, Sil: 0.904\n",
      "  Finished testing params in 0.18s\n",
      "\n",
      "[9/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40} -> ARI: 0.936, NMI: 0.933, Sil: 0.683\n",
      "  Finished testing params in 0.16s\n",
      "\n",
      "[10/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60} -> ARI: 0.936, NMI: 0.933, Sil: 0.683\n",
      "  Finished testing params in 0.16s\n",
      "\n",
      "[11/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40} -> ARI: 0.994, NMI: 0.994, Sil: 0.786\n",
      "  Finished testing params in 0.18s\n",
      "\n",
      "[12/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60} -> ARI: 0.994, NMI: 0.994, Sil: 0.786\n",
      "  Finished testing params in 0.21s\n",
      "\n",
      "[13/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40} -> ARI: 1.000, NMI: 1.000, Sil: 0.830\n",
      "  Finished testing params in 0.18s\n",
      "\n",
      "[14/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60} -> ARI: 1.000, NMI: 1.000, Sil: 0.830\n",
      "  Finished testing params in 0.16s\n",
      "\n",
      "[15/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40} -> ARI: 1.000, NMI: 1.000, Sil: 0.907\n",
      "  Finished testing params in 0.19s\n",
      "\n",
      "[16/24] Testing Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 10.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.1, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60} -> ARI: 1.000, NMI: 1.000, Sil: 0.907\n",
      "  Finished testing params in 0.18s\n",
      "\n",
      "[17/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 40} -> ARI: 0.972, NMI: 0.969, Sil: 0.670\n",
      "  Finished testing params in 0.17s\n",
      "\n",
      "[18/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 3, 'max_steps': 60} -> ARI: 0.972, NMI: 0.969, Sil: 0.670\n",
      "  Finished testing params in 0.17s\n",
      "\n",
      "[19/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 40} -> ARI: 0.978, NMI: 0.977, Sil: 0.744\n",
      "  Finished testing params in 0.20s\n",
      "\n",
      "[20/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.15, 'target_preferred_count': 5, 'max_steps': 60} -> ARI: 0.978, NMI: 0.977, Sil: 0.744\n",
      "  Finished testing params in 0.19s\n",
      "\n",
      "[21/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 40} -> ARI: 1.000, NMI: 1.000, Sil: 0.817\n",
      "  Finished testing params in 0.17s\n",
      "\n",
      "[22/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 3 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 3, 'max_steps': 60} -> ARI: 1.000, NMI: 1.000, Sil: 0.817\n",
      "  Finished testing params in 0.17s\n",
      "\n",
      "[23/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 40 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 40} -> ARI: 1.000, NMI: 1.000, Sil: 0.895\n",
      "  Finished testing params in 0.21s\n",
      "\n",
      "[24/24] Testing Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60}\n",
      "  Seed 1/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 150, Skipped: 0\n",
      "  Seed 2/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 150, Skipped: 0\n",
      "  Seed 3/3: Generating graph & simulating...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 20.0%)...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 60 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 150, Skipped: 0\n",
      "  Running clustering (kmeans) on combined data for params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60}...\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: boc\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " Processing Seed 0...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Seed 1...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      " Processing Seed 2...\n",
      "  BoC Features extracted with shape: (150, 5)\n",
      "  BoC Features + 'cosine' distance matrix calculated.\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "    Params: {'graph_noise_level': 0.2, 'edge_probability': 0.25, 'target_preferred_count': 5, 'max_steps': 60} -> ARI: 1.000, NMI: 1.000, Sil: 0.895\n",
      "  Finished testing params in 0.18s\n",
      "\n",
      "--- Sensitivity Analysis Complete ---\n",
      "Total time: 4.27s\n",
      "\n",
      "Sensitivity Analysis Results Summary:\n",
      "    graph_noise_level  edge_probability  target_preferred_count  max_steps  Num_Successful_Seeds  Avg_ARI  Std_ARI  Avg_NMI  Std_NMI  Avg_Silhouette  Std_Silhouette Error\n",
      "0                 0.0              0.15                       3         40                     3   0.9489   0.0165   0.9465   0.0183          0.6901          0.0195  None\n",
      "1                 0.0              0.15                       3         60                     3   0.9489   0.0165   0.9465   0.0183          0.6901          0.0195  None\n",
      "2                 0.0              0.15                       5         40                     3   1.0000   0.0000   1.0000   0.0000          0.7715          0.0415  None\n",
      "3                 0.0              0.15                       5         60                     3   1.0000   0.0000   1.0000   0.0000          0.7715          0.0415  None\n",
      "4                 0.0              0.25                       3         40                     3   0.9940   0.0104   0.9937   0.0110          0.8176          0.0329  None\n",
      "5                 0.0              0.25                       3         60                     3   0.9940   0.0104   0.9937   0.0110          0.8176          0.0329  None\n",
      "6                 0.0              0.25                       5         40                     3   1.0000   0.0000   1.0000   0.0000          0.9038          0.0193  None\n",
      "7                 0.0              0.25                       5         60                     3   1.0000   0.0000   1.0000   0.0000          0.9038          0.0193  None\n",
      "8                 0.1              0.15                       3         40                     3   0.9357   0.0233   0.9334   0.0242          0.6834          0.0253  None\n",
      "9                 0.1              0.15                       3         60                     3   0.9357   0.0233   0.9334   0.0242          0.6834          0.0253  None\n",
      "10                0.1              0.15                       5         40                     3   0.9945   0.0096   0.9939   0.0106          0.7860          0.0258  None\n",
      "11                0.1              0.15                       5         60                     3   0.9945   0.0096   0.9939   0.0106          0.7860          0.0258  None\n",
      "12                0.1              0.25                       3         40                     3   1.0000   0.0000   1.0000   0.0000          0.8304          0.0169  None\n",
      "13                0.1              0.25                       3         60                     3   1.0000   0.0000   1.0000   0.0000          0.8304          0.0169  None\n",
      "14                0.1              0.25                       5         40                     3   1.0000   0.0000   1.0000   0.0000          0.9067          0.0275  None\n",
      "15                0.1              0.25                       5         60                     3   1.0000   0.0000   1.0000   0.0000          0.9067          0.0275  None\n",
      "16                0.2              0.15                       3         40                     3   0.9722   0.0095   0.9693   0.0107          0.6702          0.0404  None\n",
      "17                0.2              0.15                       3         60                     3   0.9722   0.0095   0.9693   0.0107          0.6702          0.0404  None\n",
      "18                0.2              0.15                       5         40                     3   0.9778   0.0248   0.9775   0.0248          0.7437          0.0288  None\n",
      "19                0.2              0.15                       5         60                     3   0.9778   0.0248   0.9775   0.0248          0.7437          0.0288  None\n",
      "20                0.2              0.25                       3         40                     3   1.0000   0.0000   1.0000   0.0000          0.8166          0.0024  None\n",
      "21                0.2              0.25                       3         60                     3   1.0000   0.0000   1.0000   0.0000          0.8166          0.0024  None\n",
      "22                0.2              0.25                       5         40                     3   1.0000   0.0000   1.0000   0.0000          0.8950          0.0116  None\n",
      "23                0.2              0.25                       5         60                     3   1.0000   0.0000   1.0000   0.0000          0.8950          0.0116  None\n",
      "\n",
      "Full sensitivity results saved to: sensitivity_analysis_20250415_164826\\sensitivity_analysis_results_20250415_164826.csv\n",
      "\n",
      "Generating sensitivity plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_ARI_vs_graph_noise_level.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_NMI_vs_graph_noise_level.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_Silhouette_vs_graph_noise_level.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_ARI_vs_edge_probability.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_NMI_vs_edge_probability.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_Silhouette_vs_edge_probability.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_ARI_vs_target_preferred_count.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_NMI_vs_target_preferred_count.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_Silhouette_vs_target_preferred_count.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_ARI_vs_max_steps.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_NMI_vs_max_steps.png\n",
      " Saved plot: sensitivity_analysis_20250415_164826\\sensitivity_plots\\plot_Avg_Silhouette_vs_max_steps.png\n",
      "\n",
      "Sensitivity analysis script finished.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Main Execution Block -> SENSITIVITY ANALYSIS\n",
    "# ==============================================================================\n",
    "import itertools # Import for creating parameter combinations\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns # For plotting results\n",
    "import matplotlib.pyplot as plt # For plotting results\n",
    "\n",
    "\n",
    "# --- Assume all previous functions (Agent, generate..., simulate..., cluster..., plot...) are defined above ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Base Parameters (Some will be overridden in the loop) ---\n",
    "    NUM_NODES = 40\n",
    "    COLORS = ['blue', 'green', 'red', 'yellow', 'white']\n",
    "    NUM_AGENTS = 5 # Keep relatively low for faster analysis\n",
    "    NUM_TRAVERSALS_PER_AGENT = 30 # Keep relatively low for faster analysis\n",
    "    NUM_SEEDS_PER_PARAM_SET = 3 # Use fewer seeds per combo for speed during sensitivity analysis\n",
    "\n",
    "    # --- Clustering Parameters (Keep Fixed for this analysis) ---\n",
    "    N_CLUSTERS = len(COLORS)\n",
    "    FEATURE_DISTANCE_METHOD = 'boc' # Or your method of choice\n",
    "    CLUSTER_METHOD = 'kmeans'       # Or your method of choice\n",
    "    DIST_METRIC = 'cosine'          # Or your method of choice\n",
    "\n",
    "    # --- Parameters for Sensitivity Analysis ---\n",
    "    param_grid = {\n",
    "        'graph_noise_level': [0.0, 0.1, 0.2], # Example values\n",
    "        'edge_probability': [0.15, 0.25],      # Example values\n",
    "        'target_preferred_count': [3, 5],      # Example values\n",
    "        'max_steps': [40, 60]                  # Example values\n",
    "    }\n",
    "\n",
    "    # Create all combinations of parameters\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    parameter_combinations = list(itertools.product(*param_values))\n",
    "\n",
    "    print(f\"--- Starting Sensitivity Analysis ---\")\n",
    "    print(f\"Testing {len(parameter_combinations)} parameter combinations.\")\n",
    "    print(f\"Using {NUM_SEEDS_PER_PARAM_SET} simulation seeds per combination.\")\n",
    "    print(f\"Fixed Clustering: {FEATURE_DISTANCE_METHOD} / {CLUSTER_METHOD} / k={N_CLUSTERS}\")\n",
    "\n",
    "    # --- Output Directory ---\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_DIR = f\"sensitivity_analysis_{timestamp}\"\n",
    "    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Results will be saved in: {OUTPUT_DIR}\")\n",
    "\n",
    "    # --- Store Results ---\n",
    "    sensitivity_results_list = []\n",
    "    analysis_start_time = time.time()\n",
    "\n",
    "    # --- Loop through Parameter Combinations ---\n",
    "    for i, params in enumerate(parameter_combinations):\n",
    "        current_params = dict(zip(param_keys, params))\n",
    "        print(f\"\\n[{i+1}/{len(parameter_combinations)}] Testing Params: {current_params}\")\n",
    "        param_start_time = time.time()\n",
    "\n",
    "        # Store results for this parameter set across seeds\n",
    "        results_for_this_param_set = []\n",
    "\n",
    "        # --- Run simulation for multiple seeds for this param set ---\n",
    "        all_simulation_data_this_param = []\n",
    "        for seed in range(NUM_SEEDS_PER_PARAM_SET):\n",
    "            graph_attribute_seed = seed\n",
    "            graph_edge_seed = seed + NUM_SEEDS_PER_PARAM_SET # Avoid overlap if needed\n",
    "\n",
    "            print(f\"  Seed {seed+1}/{NUM_SEEDS_PER_PARAM_SET}: Generating graph & simulating...\")\n",
    "            try:\n",
    "                G = generate_structured_colored_graph(\n",
    "                    num_nodes=NUM_NODES,\n",
    "                    colors=COLORS,\n",
    "                    edge_probability=current_params['edge_probability'],\n",
    "                    structure_seed=graph_attribute_seed,\n",
    "                    edge_seed=graph_edge_seed,\n",
    "                    noise_level=current_params['graph_noise_level']\n",
    "                )\n",
    "\n",
    "                simulation_seed = seed # Use the same seed for agent simulation consistency\n",
    "                df_agents = simulate_agents(\n",
    "                    graph=G,\n",
    "                    num_agents=NUM_AGENTS,\n",
    "                    num_traversals_per_agent=NUM_TRAVERSALS_PER_AGENT,\n",
    "                    simulation_seed=simulation_seed,\n",
    "                    edge_probability=current_params['edge_probability'], # Pass current value\n",
    "                    target_preferred_count=current_params['target_preferred_count'],\n",
    "                    max_steps=current_params['max_steps'],\n",
    "                    colors_list=COLORS\n",
    "                )\n",
    "                if not df_agents.empty:\n",
    "                    all_simulation_data_this_param.append(df_agents)\n",
    "                else:\n",
    "                    print(f\"    Warning: No simulation data generated for seed {seed} with params {current_params}\")\n",
    "\n",
    "            except Exception as e_sim:\n",
    "                 print(f\"    ERROR during simulation for seed {seed}, params {current_params}: {repr(e_sim)}\")\n",
    "                 # Optionally add placeholder results or skip seed\n",
    "                 continue # Skip to next seed\n",
    "\n",
    "        if not all_simulation_data_this_param:\n",
    "            print(f\"  Skipping clustering for params {current_params}: No simulation data generated across seeds.\")\n",
    "            # Add placeholder failure result?\n",
    "            failure_result = {**current_params, 'Avg_ARI': np.nan, 'Avg_NMI': np.nan, 'Avg_Silhouette': np.nan, 'Error': 'NoSimData'}\n",
    "            sensitivity_results_list.append(failure_result)\n",
    "            continue\n",
    "\n",
    "        # Combine data from all seeds for this parameter set\n",
    "        combined_sim_data = pd.concat(all_simulation_data_this_param, ignore_index=True)\n",
    "\n",
    "        # --- Run Clustering and Evaluation (once per parameter set on combined seed data) ---\n",
    "        print(f\"  Running clustering ({CLUSTER_METHOD}) on combined data for params: {current_params}...\")\n",
    "        try:\n",
    "            summary_results_df, _ = cluster_and_evaluate(\n",
    "                df=combined_sim_data.copy(), # Use combined data\n",
    "                n_clusters=N_CLUSTERS,\n",
    "                feature_distance_method=FEATURE_DISTANCE_METHOD,\n",
    "                cluster_method=CLUSTER_METHOD,\n",
    "                dist_metric_param=DIST_METRIC\n",
    "            )\n",
    "\n",
    "            # --- Aggregate results (average over seeds if cluster_and_evaluate returns per-seed results) ---\n",
    "            # Check if summary_results_df contains multiple rows (one per seed)\n",
    "            # If cluster_and_evaluate was modified to handle pre-grouped data (by seed internally),\n",
    "            # it might return a single row with avg results.\n",
    "            # Assuming it returns multiple rows (one per seed originally passed in combined_sim_data):\n",
    "\n",
    "            if not summary_results_df.empty:\n",
    "                 # Filter out potential NaN rows if seeds failed during clustering\n",
    "                 summary_results_df = summary_results_df.dropna(subset=['ARI', 'NMI', 'Silhouette'], how='any')\n",
    "                 if not summary_results_df.empty:\n",
    "                     avg_ari = summary_results_df['ARI'].mean()\n",
    "                     avg_nmi = summary_results_df['NMI'].mean()\n",
    "                     avg_silhouette = summary_results_df['Silhouette'].mean()\n",
    "                     std_ari = summary_results_df['ARI'].std()\n",
    "                     std_nmi = summary_results_df['NMI'].std()\n",
    "                     std_silhouette = summary_results_df['Silhouette'].std()\n",
    "                     num_successful_seeds = len(summary_results_df)\n",
    "\n",
    "                     # Store the average results for this parameter combination\n",
    "                     result_summary = {\n",
    "                         **current_params, # Add the input parameters\n",
    "                         'Num_Successful_Seeds': num_successful_seeds,\n",
    "                         'Avg_ARI': avg_ari,\n",
    "                         'Std_ARI': std_ari,\n",
    "                         'Avg_NMI': avg_nmi,\n",
    "                         'Std_NMI': std_nmi,\n",
    "                         'Avg_Silhouette': avg_silhouette,\n",
    "                         'Std_Silhouette': std_silhouette,\n",
    "                         'Error': None\n",
    "                     }\n",
    "                     sensitivity_results_list.append(result_summary)\n",
    "                     print(f\"    Params: {current_params} -> ARI: {avg_ari:.3f}, NMI: {avg_nmi:.3f}, Sil: {avg_silhouette:.3f}\")\n",
    "                 else:\n",
    "                    print(f\"    Warning: Clustering evaluation failed for all seeds for params {current_params}.\")\n",
    "                    failure_result = {**current_params, 'Avg_ARI': np.nan, 'Avg_NMI': np.nan, 'Avg_Silhouette': np.nan, 'Error': 'ClusteringEvalFailed'}\n",
    "                    sensitivity_results_list.append(failure_result)\n",
    "\n",
    "            else:\n",
    "                print(f\"    Warning: Clustering evaluation produced empty results for params {current_params}.\")\n",
    "                failure_result = {**current_params, 'Avg_ARI': np.nan, 'Avg_NMI': np.nan, 'Avg_Silhouette': np.nan, 'Error': 'ClusteringEvalEmpty'}\n",
    "                sensitivity_results_list.append(failure_result)\n",
    "\n",
    "        except Exception as e_cluster:\n",
    "            print(f\"    ERROR during clustering/evaluation for params {current_params}: {repr(e_cluster)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Store failure result for this parameter combination\n",
    "            failure_result = {**current_params, 'Avg_ARI': np.nan, 'Avg_NMI': np.nan, 'Avg_Silhouette': np.nan, 'Error': str(e_cluster)}\n",
    "            sensitivity_results_list.append(failure_result)\n",
    "\n",
    "        param_end_time = time.time()\n",
    "        print(f\"  Finished testing params in {param_end_time - param_start_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Aggregate and Save Sensitivity Results ---\n",
    "    analysis_end_time = time.time()\n",
    "    print(f\"\\n--- Sensitivity Analysis Complete ---\")\n",
    "    print(f\"Total time: {analysis_end_time - analysis_start_time:.2f}s\")\n",
    "\n",
    "    if sensitivity_results_list:\n",
    "        sensitivity_df = pd.DataFrame(sensitivity_results_list)\n",
    "        print(\"\\nSensitivity Analysis Results Summary:\")\n",
    "        print(sensitivity_df.round(4).to_string())\n",
    "\n",
    "        # Save results to CSV\n",
    "        results_filename = f\"sensitivity_analysis_results_{timestamp}.csv\"\n",
    "        results_filepath = os.path.join(OUTPUT_DIR, results_filename)\n",
    "        try:\n",
    "            sensitivity_df.to_csv(results_filepath, index=False)\n",
    "            print(f\"\\nFull sensitivity results saved to: {results_filepath}\")\n",
    "        except Exception as e_save:\n",
    "            print(f\"Error saving sensitivity results: {e_save}\")\n",
    "\n",
    "        # --- Plotting Sensitivity Results ---\n",
    "        print(\"\\nGenerating sensitivity plots...\")\n",
    "        metrics_to_plot = ['Avg_ARI', 'Avg_NMI', 'Avg_Silhouette']\n",
    "        plot_dir = os.path.join(OUTPUT_DIR, \"sensitivity_plots\")\n",
    "        if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "\n",
    "        for param_name in param_keys:\n",
    "            # Identify other parameters to use as style/hue (if more than one varies)\n",
    "            other_params = [p for p in param_keys if p != param_name]\n",
    "            hue_param = other_params[0] if other_params else None\n",
    "            style_param = other_params[1] if len(other_params) > 1 else None\n",
    "\n",
    "            for metric in metrics_to_plot:\n",
    "                if metric not in sensitivity_df.columns or sensitivity_df[metric].isnull().all():\n",
    "                    print(f\"Skipping plot for {metric} vs {param_name} (metric data missing or all NaN)\")\n",
    "                    continue\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                try:\n",
    "                    sns.lineplot(\n",
    "                        data=sensitivity_df,\n",
    "                        x=param_name,\n",
    "                        y=metric,\n",
    "                        hue=hue_param, # Color lines by another parameter\n",
    "                        style=style_param, # Change line style by a third parameter\n",
    "                        marker='o',\n",
    "                        err_style=\"band\" # Show std dev band if StdDev columns exist (requires CI calculation or passing std dev)\n",
    "                        # You might need to calculate CI manually or adjust plotter if using precomputed std dev columns\n",
    "                        # errorbar=('sd', 1) # If using seaborn >= 0.12 might work with std dev columns\n",
    "                    )\n",
    "                    title = f'{metric} vs {param_name}'\n",
    "                    if hue_param: title += f' (Colored by {hue_param})'\n",
    "                    if style_param: title += f' (Styled by {style_param})'\n",
    "                    plt.title(title)\n",
    "                    plt.xlabel(param_name)\n",
    "                    plt.ylabel(metric)\n",
    "                    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "                    if hue_param: # Place legend outside plot\n",
    "                         plt.legend(title=hue_param, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                    plt.tight_layout(rect=[0, 0, 0.85 if hue_param else 1, 1]) # Adjust layout for legend\n",
    "\n",
    "                    plot_filename = f\"plot_{metric}_vs_{param_name}.png\"\n",
    "                    plot_filepath = os.path.join(plot_dir, plot_filename)\n",
    "                    plt.savefig(plot_filepath)\n",
    "                    print(f\" Saved plot: {plot_filepath}\")\n",
    "                    plt.close()\n",
    "\n",
    "                except Exception as e_plot:\n",
    "                    print(f\" ERROR generating plot for {metric} vs {param_name}: {repr(e_plot)}\")\n",
    "                    plt.close() # Close plot even if error occurred\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No sensitivity results were generated.\")\n",
    "\n",
    "    print(\"\\nSensitivity analysis script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01d514-9b04-426b-84fa-a89027ce20cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cfa91-f065-4d0c-a6b6-96f988c40d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Link Between Clusters and Preferences: How are the \"preferences\" formally defined, and what is the explicit mechanism or hypothesis \n",
    "linking the k-means clusters of trajectories to these specific preferences? How does the classifier learn or establish this link, \n",
    "and crucially, how is this mapping validated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56422470-c4c7-49b5-8b5d-572a49da7b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf397959-8154-4a00-871a-16301a125ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02222a6b-b03d-4cb4-ad47-59aa524e1b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9833f-0bbb-4f6e-a0d4-898ff7db2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choice of Algorithms and Parameters\n",
    "Question: Why were k-means and the specific chosen classifier selected over other clustering (e.g., DBSCAN, hierarchical) and \n",
    "classification algorithms? How sensitive are the final results (cluster quality, preference classification accuracy) \n",
    "to the choice of 'k' in k-means, the distance metric used for trajectories, and the hyperparameters of the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563850ef-66f6-404a-8626-6c2d847b41ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47446255-7743-41a8-a05b-26ef5a87ed97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349e077-bcba-4706-8cc7-188613d1315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline Comparison and Performance Evaluation:\n",
    "Question: How does the performance of your entire pipeline compare against established or simpler baseline methods for inferring preferences \n",
    "in your specific domain (if they exist)? What quantitative metrics are used to evaluate the accuracy or validity of the final inferred preferences, \n",
    "not just intermediate steps like clustering quality or classifier accuracy on potentially simulated labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced7b52-4847-434a-85fc-a8572ffa6bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a6fad-b2bf-4332-afc0-62358de127a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d7fe3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed</th>\n",
       "      <th>Agent</th>\n",
       "      <th>Start Node</th>\n",
       "      <th>Color Preference</th>\n",
       "      <th>Target Preferred Count</th>\n",
       "      <th>Max Steps</th>\n",
       "      <th>Actual Steps Taken</th>\n",
       "      <th>Preferred Nodes Found</th>\n",
       "      <th>Path Length</th>\n",
       "      <th>Preferred Color Path Count</th>\n",
       "      <th>Preferred_Color_Proportion</th>\n",
       "      <th>Path</th>\n",
       "      <th>Mapped Colors</th>\n",
       "      <th>Visited Shapes</th>\n",
       "      <th>Visited Shininess</th>\n",
       "      <th>Density</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Cluster_Remapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>yellow</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>[25, 37, 29, 19, 10, 22, 17, 12]</td>\n",
       "      <td>[white, yellow, yellow, yellow, yellow, blue, white, yellow]</td>\n",
       "      <td>[circle, circle, square, triangle, square, square, triangle, square]</td>\n",
       "      <td>[not_shiny, not_shiny, not_shiny, shiny, shiny, not_shiny, shiny, shiny]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>green</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[38, 15, 8, 27, 6, 21]</td>\n",
       "      <td>[green, green, green, white, green, green]</td>\n",
       "      <td>[triangle, square, triangle, triangle, square, triangle]</td>\n",
       "      <td>[shiny, shiny, shiny, not_shiny, shiny, shiny]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[40, 18, 34, 8, 9, 23]</td>\n",
       "      <td>[red, red, red, green, red, red]</td>\n",
       "      <td>[square, square, triangle, triangle, triangle, circle]</td>\n",
       "      <td>[not_shiny, not_shiny, not_shiny, shiny, not_shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[31, 3, 33, 26, 27, 16, 40, 11, 38, 32]</td>\n",
       "      <td>[yellow, white, white, white, white, blue, red, green, green, white]</td>\n",
       "      <td>[triangle, square, square, circle, triangle, square, square, triangle, triangle, triangle]</td>\n",
       "      <td>[not_shiny, not_shiny, shiny, not_shiny, not_shiny, shiny, not_shiny, shiny, shiny, shiny]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[29, 20, 33, 32, 3, 5, 27]</td>\n",
       "      <td>[yellow, white, white, white, white, yellow, white]</td>\n",
       "      <td>[square, square, square, triangle, square, circle, triangle]</td>\n",
       "      <td>[not_shiny, shiny, shiny, shiny, not_shiny, shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Seed  Agent  Start Node Color Preference  Target Preferred Count  Max Steps  Actual Steps Taken  \\\n",
       "0     0      0          25           yellow                       5         50                   7   \n",
       "1     0      1          38            green                       5         50                   5   \n",
       "2     0      2          40              red                       5         50                   5   \n",
       "3     0      3          31            white                       5         50                   9   \n",
       "4     0      4          29            white                       5         50                   6   \n",
       "\n",
       "   Preferred Nodes Found  Path Length  Preferred Color Path Count  Preferred_Color_Proportion  \\\n",
       "0                      5            8                           5                    0.625000   \n",
       "1                      5            6                           5                    0.833333   \n",
       "2                      5            6                           5                    0.833333   \n",
       "3                      5           10                           5                    0.500000   \n",
       "4                      5            7                           5                    0.714286   \n",
       "\n",
       "                                      Path                                                         Mapped Colors  \\\n",
       "0         [25, 37, 29, 19, 10, 22, 17, 12]          [white, yellow, yellow, yellow, yellow, blue, white, yellow]   \n",
       "1                   [38, 15, 8, 27, 6, 21]                            [green, green, green, white, green, green]   \n",
       "2                   [40, 18, 34, 8, 9, 23]                                      [red, red, red, green, red, red]   \n",
       "3  [31, 3, 33, 26, 27, 16, 40, 11, 38, 32]  [yellow, white, white, white, white, blue, red, green, green, white]   \n",
       "4               [29, 20, 33, 32, 3, 5, 27]                   [yellow, white, white, white, white, yellow, white]   \n",
       "\n",
       "                                                                               Visited Shapes  \\\n",
       "0                        [circle, circle, square, triangle, square, square, triangle, square]   \n",
       "1                                    [triangle, square, triangle, triangle, square, triangle]   \n",
       "2                                      [square, square, triangle, triangle, triangle, circle]   \n",
       "3  [triangle, square, square, circle, triangle, square, square, triangle, triangle, triangle]   \n",
       "4                                [square, square, square, triangle, square, circle, triangle]   \n",
       "\n",
       "                                                                            Visited Shininess  Density  Cluster  \\\n",
       "0                    [not_shiny, not_shiny, not_shiny, shiny, shiny, not_shiny, shiny, shiny]      0.2        5   \n",
       "1                                              [shiny, shiny, shiny, not_shiny, shiny, shiny]      0.2        2   \n",
       "2                              [not_shiny, not_shiny, not_shiny, shiny, not_shiny, not_shiny]      0.2        4   \n",
       "3  [not_shiny, not_shiny, shiny, not_shiny, not_shiny, shiny, not_shiny, shiny, shiny, shiny]      0.2        3   \n",
       "4                               [not_shiny, shiny, shiny, shiny, not_shiny, shiny, not_shiny]      0.2        3   \n",
       "\n",
       "   Cluster_Remapped  \n",
       "0                 3  \n",
       "1                 1  \n",
       "2                 4  \n",
       "3                 5  \n",
       "4                 5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5262dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved graph plot to clustering_results_sequence_dl_lstm32_embed16_k5\\graphs\\graph_seed_4.png\n"
     ]
    }
   ],
   "source": [
    "# Plot the graph \n",
    "plot_colored_graph(G, seed, output_dir=os.path.join(OUTPUT_DIR, \"graphs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c33dcad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed</th>\n",
       "      <th>Agent</th>\n",
       "      <th>Start Node</th>\n",
       "      <th>Color Preference</th>\n",
       "      <th>Target Preferred Count</th>\n",
       "      <th>Max Steps</th>\n",
       "      <th>Actual Steps Taken</th>\n",
       "      <th>Preferred Nodes Found</th>\n",
       "      <th>Path Length</th>\n",
       "      <th>Preferred Color Path Count</th>\n",
       "      <th>Preferred_Color_Proportion</th>\n",
       "      <th>Path</th>\n",
       "      <th>Mapped Colors</th>\n",
       "      <th>Visited Shapes</th>\n",
       "      <th>Visited Shininess</th>\n",
       "      <th>Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[50, 5, 44, 74, 73, 71]</td>\n",
       "      <td>[yellow, white, white, white, white, white]</td>\n",
       "      <td>[square, circle, triangle, triangle, circle, square]</td>\n",
       "      <td>[not_shiny, shiny, shiny, shiny, not_shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[62, 49, 77, 78, 41, 4]</td>\n",
       "      <td>[yellow, red, red, red, red, red]</td>\n",
       "      <td>[circle, triangle, square, square, triangle, square]</td>\n",
       "      <td>[shiny, not_shiny, not_shiny, not_shiny, shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>yellow</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[13, 39, 54, 46, 7, 63]</td>\n",
       "      <td>[yellow, yellow, yellow, yellow, white, yellow]</td>\n",
       "      <td>[circle, circle, square, circle, square, triangle]</td>\n",
       "      <td>[shiny, not_shiny, shiny, shiny, not_shiny, shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[10, 17, 4, 40, 70, 59, 23]</td>\n",
       "      <td>[white, red, red, red, red, white, red]</td>\n",
       "      <td>[square, triangle, square, square, triangle, square, circle]</td>\n",
       "      <td>[shiny, shiny, not_shiny, not_shiny, shiny, not_shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[71, 47, 58, 27, 73, 74]</td>\n",
       "      <td>[white, white, white, green, white, white]</td>\n",
       "      <td>[square, triangle, square, triangle, circle, triangle]</td>\n",
       "      <td>[not_shiny, shiny, not_shiny, not_shiny, not_shiny, shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>blue</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[67, 72, 69, 17, 37, 10]</td>\n",
       "      <td>[green, blue, blue, blue, blue, blue]</td>\n",
       "      <td>[circle, circle, square, circle, circle, circle]</td>\n",
       "      <td>[not_shiny, not_shiny, shiny, not_shiny, shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[13, 14, 74, 76, 3]</td>\n",
       "      <td>[red, red, red, red, red]</td>\n",
       "      <td>[square, square, square, square, triangle]</td>\n",
       "      <td>[not_shiny, shiny, not_shiny, not_shiny, shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>green</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[38, 20, 41, 32, 62, 31]</td>\n",
       "      <td>[white, green, green, green, green, green]</td>\n",
       "      <td>[square, circle, square, square, square, circle]</td>\n",
       "      <td>[shiny, shiny, shiny, shiny, shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[23, 25, 33, 55, 38, 45]</td>\n",
       "      <td>[green, white, white, white, white, white]</td>\n",
       "      <td>[triangle, triangle, triangle, square, square, square]</td>\n",
       "      <td>[not_shiny, not_shiny, shiny, not_shiny, shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>blue</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[75, 12, 17, 21, 39, 29]</td>\n",
       "      <td>[green, blue, blue, blue, blue, blue]</td>\n",
       "      <td>[triangle, square, circle, circle, circle, circle]</td>\n",
       "      <td>[not_shiny, not_shiny, not_shiny, shiny, not_shiny, not_shiny]</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Seed  Agent  Start Node Color Preference  Target Preferred Count  Max Steps  Actual Steps Taken  \\\n",
       "0        0      0          50            white                       5         50                   5   \n",
       "1        0      1          62              red                       5         50                   5   \n",
       "2        0      2          13           yellow                       5         50                   5   \n",
       "3        0      3          10              red                       5         50                   6   \n",
       "4        0      4          71            white                       5         50                   5   \n",
       "...    ...    ...         ...              ...                     ...        ...                 ...   \n",
       "4995     9      5          67             blue                       5         50                   5   \n",
       "4996     9      6          13              red                       5         50                   4   \n",
       "4997     9      7          38            green                       5         50                   5   \n",
       "4998     9      8          23            white                       5         50                   5   \n",
       "4999     9      9          75             blue                       5         50                   5   \n",
       "\n",
       "      Preferred Nodes Found  Path Length  Preferred Color Path Count  Preferred_Color_Proportion  \\\n",
       "0                         5            6                           5                    0.833333   \n",
       "1                         5            6                           5                    0.833333   \n",
       "2                         5            6                           5                    0.833333   \n",
       "3                         5            7                           5                    0.714286   \n",
       "4                         5            6                           5                    0.833333   \n",
       "...                     ...          ...                         ...                         ...   \n",
       "4995                      5            6                           5                    0.833333   \n",
       "4996                      5            5                           5                    1.000000   \n",
       "4997                      5            6                           5                    0.833333   \n",
       "4998                      5            6                           5                    0.833333   \n",
       "4999                      5            6                           5                    0.833333   \n",
       "\n",
       "                             Path                                    Mapped Colors  \\\n",
       "0         [50, 5, 44, 74, 73, 71]      [yellow, white, white, white, white, white]   \n",
       "1         [62, 49, 77, 78, 41, 4]                [yellow, red, red, red, red, red]   \n",
       "2         [13, 39, 54, 46, 7, 63]  [yellow, yellow, yellow, yellow, white, yellow]   \n",
       "3     [10, 17, 4, 40, 70, 59, 23]          [white, red, red, red, red, white, red]   \n",
       "4        [71, 47, 58, 27, 73, 74]       [white, white, white, green, white, white]   \n",
       "...                           ...                                              ...   \n",
       "4995     [67, 72, 69, 17, 37, 10]            [green, blue, blue, blue, blue, blue]   \n",
       "4996          [13, 14, 74, 76, 3]                        [red, red, red, red, red]   \n",
       "4997     [38, 20, 41, 32, 62, 31]       [white, green, green, green, green, green]   \n",
       "4998     [23, 25, 33, 55, 38, 45]       [green, white, white, white, white, white]   \n",
       "4999     [75, 12, 17, 21, 39, 29]            [green, blue, blue, blue, blue, blue]   \n",
       "\n",
       "                                                    Visited Shapes  \\\n",
       "0             [square, circle, triangle, triangle, circle, square]   \n",
       "1             [circle, triangle, square, square, triangle, square]   \n",
       "2               [circle, circle, square, circle, square, triangle]   \n",
       "3     [square, triangle, square, square, triangle, square, circle]   \n",
       "4           [square, triangle, square, triangle, circle, triangle]   \n",
       "...                                                            ...   \n",
       "4995              [circle, circle, square, circle, circle, circle]   \n",
       "4996                    [square, square, square, square, triangle]   \n",
       "4997              [square, circle, square, square, square, circle]   \n",
       "4998        [triangle, triangle, triangle, square, square, square]   \n",
       "4999            [triangle, square, circle, circle, circle, circle]   \n",
       "\n",
       "                                                      Visited Shininess  Density  \n",
       "0                [not_shiny, shiny, shiny, shiny, not_shiny, not_shiny]      0.2  \n",
       "1            [shiny, not_shiny, not_shiny, not_shiny, shiny, not_shiny]      0.2  \n",
       "2                    [shiny, not_shiny, shiny, shiny, not_shiny, shiny]      0.2  \n",
       "3     [shiny, shiny, not_shiny, not_shiny, shiny, not_shiny, not_shiny]      0.2  \n",
       "4            [not_shiny, shiny, not_shiny, not_shiny, not_shiny, shiny]      0.2  \n",
       "...                                                                 ...      ...  \n",
       "4995         [not_shiny, not_shiny, shiny, not_shiny, shiny, not_shiny]      0.2  \n",
       "4996                    [not_shiny, shiny, not_shiny, not_shiny, shiny]      0.2  \n",
       "4997                     [shiny, shiny, shiny, shiny, shiny, not_shiny]      0.2  \n",
       "4998         [not_shiny, not_shiny, shiny, not_shiny, shiny, not_shiny]      0.2  \n",
       "4999     [not_shiny, not_shiny, not_shiny, shiny, not_shiny, not_shiny]      0.2  \n",
       "\n",
       "[5000 rows x 16 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_simulation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6df657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Classifier Accuracy on Test Set: 0.4467\n",
      "\n",
      "--- Top Predictors (Features) for Clusters ---\n",
      "  feature  importance\n",
      "0    blue    0.229053\n",
      "3   white    0.216498\n",
      "2     red    0.201647\n",
      "1   green    0.186894\n",
      "4  yellow    0.165908\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder # If cluster labels are not numeric\n",
    "\n",
    "# 1. Prepare Data\n",
    "valid_data = data_with_clusters[data_with_clusters['Cluster'] > 0].copy()\n",
    "\n",
    "# Target Variable\n",
    "y = valid_data['Cluster']\n",
    "\n",
    "# 2. Prepare Features (Using BoC)\n",
    "corpus = valid_data['Mapped Colors'].apply(lambda colors: ' '.join(map(str, colors if isinstance(colors, list) else [])))\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 3. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Choose and Train Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced') \n",
    "print(\"Training classifier...\")\n",
    "classifier.fit(X_train, y_train)\n",
    "print(f\"Classifier Accuracy on Test Set: {classifier.score(X_test, y_test):.4f}\") \n",
    "\n",
    "# 5. Extract and Analyze Feature Importances\n",
    "importances = classifier.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top Predictors (Features) for Clusters ---\")\n",
    "print(feature_importance_df.head(15)) # Display top 10 overall predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5ed02e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\taodz\\Desktop\\PhD_Preferences_2022\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(\"Current working directory:\", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559da721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba21bb7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Simulation Phase ---\n",
      "\n",
      "Generating structured graph for Seed 0...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 0...\n",
      " Starting simulations for Seed 0...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 30 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 0. Completed: 100, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 1...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 1...\n",
      " Starting simulations for Seed 1...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 30 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 1. Completed: 100, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 2...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 2...\n",
      " Starting simulations for Seed 2...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 30 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 2. Completed: 100, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 3...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 3...\n",
      " Starting simulations for Seed 3...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 30 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 3. Completed: 100, Skipped: 0\n",
      "\n",
      "Generating structured graph for Seed 4...\n",
      "  Assigning 40 nodes to 5 color regions (Noise: 0.0%)...\n",
      "Running Simulation for Seed 4...\n",
      " Starting simulations for Seed 4...\n",
      "  NOTE: Agent goal: Find 5 preferred nodes (max 30 steps).\n",
      "  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\n",
      "  Finished simulations for Seed 4. Completed: 100, Skipped: 0\n",
      "\n",
      "--- Simulation Phase Complete ---\n",
      "Generated 500 records in 0.08s.\n",
      "Path Length Stats:\n",
      "count    500.000000\n",
      "mean       7.666000\n",
      "std        1.651635\n",
      "min        5.000000\n",
      "25%        6.000000\n",
      "50%        7.000000\n",
      "75%        8.000000\n",
      "max       16.000000\n",
      "Name: Path Length, dtype: float64\n",
      "\n",
      "--- Starting Clustering Phase using sequence_dl features ---\n",
      "\n",
      "--- Starting Clustering Evaluation ---\n",
      " Feature/Distance Method: sequence_dl\n",
      " Clustering Algorithm: kmeans\n",
      " Target Clusters (k): 5\n",
      " DL Params: MaxLen=35, Embed=16, LSTM=32\n",
      " Processing Seed 0...\n",
      "  Extracting Sequence Features using DL (MaxLen=35, Embed=16, LSTM=32)...\n",
      "   - Padded sequences shape: (100, 35)\n",
      "   - Predicting features with DL model...\n",
      "   - Extracted DL Features shape: (100, 32)\n",
      "  Sequence DL features extracted. Associated metric for clustering: 'euclidean'\n",
      "  Running kmeans Clustering...\n",
      "   - Found 5 unique clusters.\n",
      "   - Calculating Silhouette Score (using metric: 'euclidean')...\n",
      "     (Using extracted features)\n",
      "     Silhouette Score: 0.6931\n",
      "   - Calculating ARI and NMI...\n",
      "     ARI: 1.0000, NMI: 1.0000\n",
      " Processing Seed 1...\n",
      "  Extracting Sequence Features using DL (MaxLen=35, Embed=16, LSTM=32)...\n",
      "   - Padded sequences shape: (100, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Predicting features with DL model...\n",
      "   - Extracted DL Features shape: (100, 32)\n",
      "  Sequence DL features extracted. Associated metric for clustering: 'euclidean'\n",
      "  Running kmeans Clustering...\n",
      "   - Found 5 unique clusters.\n",
      "   - Calculating Silhouette Score (using metric: 'euclidean')...\n",
      "     (Using extracted features)\n",
      "     Silhouette Score: 0.6536\n",
      "   - Calculating ARI and NMI...\n",
      "     ARI: 1.0000, NMI: 1.0000\n",
      " Processing Seed 2...\n",
      "  Extracting Sequence Features using DL (MaxLen=35, Embed=16, LSTM=32)...\n",
      "   - Padded sequences shape: (100, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Predicting features with DL model...\n",
      "   - Extracted DL Features shape: (100, 32)\n",
      "  Sequence DL features extracted. Associated metric for clustering: 'euclidean'\n",
      "  Running kmeans Clustering...\n",
      "   - Found 5 unique clusters.\n",
      "   - Calculating Silhouette Score (using metric: 'euclidean')...\n",
      "     (Using extracted features)\n",
      "     Silhouette Score: 0.6361\n",
      "   - Calculating ARI and NMI...\n",
      "     ARI: 1.0000, NMI: 1.0000\n",
      " Processing Seed 3...\n",
      "  Extracting Sequence Features using DL (MaxLen=35, Embed=16, LSTM=32)...\n",
      "   - Padded sequences shape: (100, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Predicting features with DL model...\n",
      "   - Extracted DL Features shape: (100, 32)\n",
      "  Sequence DL features extracted. Associated metric for clustering: 'euclidean'\n",
      "  Running kmeans Clustering...\n",
      "   - Found 5 unique clusters.\n",
      "   - Calculating Silhouette Score (using metric: 'euclidean')...\n",
      "     (Using extracted features)\n",
      "     Silhouette Score: 0.5903\n",
      "   - Calculating ARI and NMI...\n",
      "     ARI: 0.9514, NMI: 0.9508\n",
      " Processing Seed 4...\n",
      "  Extracting Sequence Features using DL (MaxLen=35, Embed=16, LSTM=32)...\n",
      "   - Padded sequences shape: (100, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Predicting features with DL model...\n",
      "   - Extracted DL Features shape: (100, 32)\n",
      "  Sequence DL features extracted. Associated metric for clustering: 'euclidean'\n",
      "  Running kmeans Clustering...\n",
      "   - Found 5 unique clusters.\n",
      "   - Calculating Silhouette Score (using metric: 'euclidean')...\n",
      "     (Using extracted features)\n",
      "     Silhouette Score: 0.7169\n",
      "   - Calculating ARI and NMI...\n",
      "     ARI: 1.0000, NMI: 1.0000\n",
      "\n",
      "Clustering and evaluation complete.\n",
      "\n",
      "--- Clustering Phase Complete ---\n",
      "Completed in 15.87s.\n",
      "\n",
      "--- Clustering Summary Results ---\n",
      "   Seed Algorithm          Feature_Metric  Num_Clusters_Target  Num_Clusters_Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Cluster_Color_Percentage  Silhouette     ARI     NMI  Max_Color_Separation\n",
      "0     0    kmeans  sequence_dl(euclidean)                    5                   5       {1: {'blue': '8.06%', 'green': '4.84%', 'red': '76.61%', 'white': '5.65%', 'yellow': '4.84%'}, 2: {'blue': '5.34%', 'green': '6.11%', 'red': '4.58%', 'white': '80.15%', 'yellow': '3.82%'}, 3: {'blue': '6.25%', 'green': '13.02%', 'red': '4.69%', 'white': '5.73%', 'yellow': '70.31%'}, 4: {'blue': '48.78%', 'green': '10.73%', 'red': '15.12%', 'white': '13.66%', 'yellow': '11.71%'}, 5: {'blue': '6.67%', 'green': '61.90%', 'red': '10.48%', 'white': '13.33%', 'yellow': '7.62%'}}      0.6931  1.0000  1.0000               76.3359\n",
      "1     1    kmeans  sequence_dl(euclidean)                    5                   5         {1: {'blue': '6.16%', 'green': '5.48%', 'red': '10.96%', 'white': '68.49%', 'yellow': '8.90%'}, 2: {'blue': '65.48%', 'green': '10.71%', 'red': '8.33%', 'white': '8.33%', 'yellow': '7.14%'}, 3: {'blue': '5.22%', 'green': '73.91%', 'red': '6.96%', 'white': '5.22%', 'yellow': '8.70%'}, 4: {'blue': '10.98%', 'green': '8.09%', 'red': '60.69%', 'white': '5.20%', 'yellow': '15.03%'}, 5: {'blue': '2.84%', 'green': '7.09%', 'red': '7.80%', 'white': '11.35%', 'yellow': '70.92%'}}      0.6536  1.0000  1.0000               68.6957\n",
      "2     2    kmeans  sequence_dl(euclidean)                    5                   5          {1: {'blue': '5.79%', 'green': '9.47%', 'red': '12.63%', 'white': '63.16%', 'yellow': '8.95%'}, 2: {'blue': '76.92%', 'green': '5.13%', 'red': '5.13%', 'white': '5.13%', 'yellow': '7.69%'}, 3: {'blue': '6.40%', 'green': '7.20%', 'red': '68.00%', 'white': '10.40%', 'yellow': '8.00%'}, 4: {'blue': '8.67%', 'green': '9.33%', 'red': '12.67%', 'white': '6.00%', 'yellow': '63.33%'}, 5: {'blue': '6.67%', 'green': '66.67%', 'red': '12.73%', 'white': '5.45%', 'yellow': '8.48%'}}      0.6361  1.0000  1.0000               71.7949\n",
      "3     3    kmeans  sequence_dl(euclidean)                    5                   5  {1: {'blue': '12.92%', 'green': '8.99%', 'red': '15.73%', 'white': '12.36%', 'yellow': '50.00%'}, 2: {'blue': '65.07%', 'green': '9.59%', 'red': '6.16%', 'white': '10.27%', 'yellow': '8.90%'}, 3: {'blue': '10.19%', 'green': '60.51%', 'red': '11.46%', 'white': '5.73%', 'yellow': '12.10%'}, 4: {'blue': '6.84%', 'green': '14.74%', 'red': '55.26%', 'white': '10.53%', 'yellow': '12.63%'}, 5: {'blue': '6.15%', 'green': '9.50%', 'red': '11.17%', 'white': '61.45%', 'yellow': '11.73%'}}      0.5903  0.9514  0.9508               58.9041\n",
      "4     4    kmeans  sequence_dl(euclidean)                    5                   5        {1: {'blue': '10.98%', 'green': '6.94%', 'red': '10.40%', 'white': '63.58%', 'yellow': '8.09%'}, 2: {'blue': '10.57%', 'green': '62.26%', 'red': '11.32%', 'white': '11.32%', 'yellow': '4.53%'}, 3: {'blue': '6.08%', 'green': '12.16%', 'red': '70.95%', 'white': '4.73%', 'yellow': '6.08%'}, 4: {'blue': '83.33%', 'green': '5.56%', 'red': '0.93%', 'white': '4.63%', 'yellow': '5.56%'}, 5: {'blue': '7.14%', 'green': '7.14%', 'red': '4.76%', 'white': '9.52%', 'yellow': '71.43%'}}      0.7169  1.0000  1.0000               82.4074\n",
      "--------------------------------------------------\n",
      "Avg ARI: 0.9903\n",
      "Avg NMI: 0.9902\n",
      "Avg Silhouette: 0.6580\n",
      "--------------------------------------------------\n",
      "Summary results saved to clustering_results_sequence_dl_lstm32_embed16_k5\\summary_sequence_dl_lstm32_k5.csv\n",
      "\n",
      "--- Remapping cluster labels for optimal visualization using Hungarian algorithm ---\n",
      "  Target Plot Order Mapping: {'blue': 1, 'green': 2, 'red': 3, 'white': 4, 'yellow': 5}\n",
      "  Raw Confusion Matrix (Rows: True Target Int, Cols: Based on Observed Pred Labels):\n",
      "[[ 1 59  0 38  0]\n",
      " [ 0 33 36  0 35]\n",
      " [20  0 38 42  0]\n",
      " [66 21  0  0 22]\n",
      " [17  0 27 19 26]]\n",
      "  Optimal Assignment (True Label Index -> Assigned Original Cluster Index):\n",
      "    True 'blue' (Target Int 1) best matches Original Cluster Label 2\n",
      "    True 'green' (Target Int 2) best matches Original Cluster Label 3\n",
      "    True 'red' (Target Int 3) best matches Original Cluster Label 4\n",
      "    True 'white' (Target Int 4) best matches Original Cluster Label 1\n",
      "    True 'yellow' (Target Int 5) best matches Original Cluster Label 5\n",
      "  Cluster Remapping Dict (Original Cluster -> Target Plot Int): {2: 1, 3: 2, 4: 3, 1: 4, 5: 5}\n",
      "\n",
      "--- Generating Confusion Matrix (using OPTIMALLY REMAPPED cluster labels) ---\n",
      "\n",
      "--- Confusion Matrix Generation (Cluster_Remapped vs Color Preference) ---\n",
      " Using True Color Mapping for Plot Rows: {'blue': 1, 'green': 2, 'red': 3, 'white': 4, 'yellow': 5}\n",
      " Using Predicted Cluster Labels for Plot Columns: [1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taodz\\miniconda3\\envs\\py_project_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved confusion matrix plot to clustering_results_sequence_dl_lstm32_embed16_k5\\plots_remapped_optimal\\confusion_matrix_Cluster_Remapped_vs_Color Preference.png\n",
      " Renamed plot to confusion_matrix_OPTIMAL_REORDERED_sequence_dl_kmeans_k5.png\n",
      "\n",
      "Pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Imports\n",
    "# ==============================================================================\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Set, Tuple, Any, Optional\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack as sparse_hstack\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment # For optimal remapping\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# <<< NEW IMPORTS for Deep Learning Sequence Processing >>>\n",
    "import tensorflow as tf\n",
    "# No need for 'from tensorflow import keras' usually\n",
    "# Import components directly from tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Masking, Input\n",
    "from tensorflow.keras.utils import pad_sequences # <<< Corrected Import\n",
    "# <<< END NEW IMPORTS >>>\n",
    "\n",
    "# Set random seed for TensorFlow for reproducibility (optional)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ==============================================================================\n",
    "# Agent Definition (Agent V5.1 - Unchanged)\n",
    "# ==============================================================================\n",
    "class Agent:\n",
    "    \"\"\" Agent with Preference-Driven Goal: Find N preferred nodes using\n",
    "        Deterministic Preference strategy (always choose preferred if available). \"\"\"\n",
    "    def __init__(self, start_node: int, color_preference: str,\n",
    "                 target_preferred_count: int, max_steps: int):\n",
    "        self.start_node: int = start_node; self.color_preference: str = color_preference\n",
    "        self.target_preferred_count: int = target_preferred_count; self.max_steps: int = max_steps\n",
    "        self.current_node: int = start_node; self.visited_nodes: Set[int] = {start_node}\n",
    "        self.path: List[int] = [start_node]; self.preferred_nodes_visited: Set[int] = set()\n",
    "        self.visited_shapes: List[str] = []; self.visited_shininess: List[str] = []\n",
    "\n",
    "    def find_next_node(self, graph: nx.Graph, rng: random.Random) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Determines the next node using a DETERMINISTIC PREFERENCE strategy.\n",
    "        (Using explicit loop for robustness against potential NameError).\n",
    "        \"\"\"\n",
    "        neighbors = list(graph.neighbors(self.current_node))\n",
    "        valid_neighbors = [n for n in neighbors if n in graph]\n",
    "        unvisited_neighbors = [n for n in valid_neighbors if n not in self.visited_nodes]\n",
    "\n",
    "        if not unvisited_neighbors:\n",
    "            return None # Stuck\n",
    "\n",
    "        preferred_unvisited_neighbors = []\n",
    "        non_preferred_unvisited_neighbors = [] # Keep track for the else case\n",
    "\n",
    "        for neigh_node in unvisited_neighbors:\n",
    "            try:\n",
    "                node_attrs = graph.nodes.get(neigh_node, {})\n",
    "                node_color = node_attrs.get('color')\n",
    "                if node_color == self.color_preference:\n",
    "                    preferred_unvisited_neighbors.append(neigh_node)\n",
    "                else:\n",
    "                    non_preferred_unvisited_neighbors.append(neigh_node)\n",
    "            except Exception as e_inner:\n",
    "                print(f\"  ERROR accessing node {neigh_node} attributes: {repr(e_inner)}\")\n",
    "                non_preferred_unvisited_neighbors.append(neigh_node)\n",
    "\n",
    "        if preferred_unvisited_neighbors:\n",
    "            return rng.choice(preferred_unvisited_neighbors)\n",
    "        elif non_preferred_unvisited_neighbors:\n",
    "            return rng.choice(non_preferred_unvisited_neighbors)\n",
    "        else:\n",
    "            print(f\" Warning: Agent at {self.current_node} has no valid unvisited neighbors to move to. Stuck.\")\n",
    "            return None # Truly stuck\n",
    "\n",
    "    def traverse_graph(self, graph: nx.Graph, rng: random.Random) -> Tuple[List[int], int, int]:\n",
    "        \"\"\" Simulates exploration walk using deterministic preference strategy. \"\"\"\n",
    "        steps_taken = 0; self.preferred_nodes_visited = set()\n",
    "        try:\n",
    "            start_node_attrs = graph.nodes[self.start_node]; self.visited_shapes.append(start_node_attrs.get('shape', 'unknown')); self.visited_shininess.append(start_node_attrs.get('shiny', 'unknown'))\n",
    "            if start_node_attrs.get('color') == self.color_preference: self.preferred_nodes_visited.add(self.start_node)\n",
    "        except KeyError: self.visited_shapes.append('unknown'); self.visited_shininess.append('unknown')\n",
    "\n",
    "        while len(self.preferred_nodes_visited) < self.target_preferred_count and steps_taken < self.max_steps:\n",
    "            next_node = self.find_next_node(graph, rng)\n",
    "            if next_node is None: break\n",
    "            self.visited_nodes.add(next_node); self.path.append(next_node); self.current_node = next_node; steps_taken += 1\n",
    "            try:\n",
    "                node_attrs = graph.nodes[next_node]; self.visited_shapes.append(node_attrs.get('shape', 'unknown')); self.visited_shininess.append(node_attrs.get('shiny', 'unknown'))\n",
    "                if node_attrs.get('color') == self.color_preference: self.preferred_nodes_visited.add(next_node)\n",
    "            except KeyError: self.visited_shapes.append('unknown'); self.visited_shininess.append('unknown')\n",
    "\n",
    "        preferred_nodes_found_count = len(self.preferred_nodes_visited)\n",
    "        return self.path, preferred_nodes_found_count, steps_taken\n",
    "\n",
    "# ==============================================================================\n",
    "# Graph Generation (Unchanged)\n",
    "# ==============================================================================\n",
    "def generate_structured_colored_graph(num_nodes: int, colors: List[str], edge_probability: float, structure_seed: int, edge_seed: int, noise_level: float = 0.1) -> nx.Graph:\n",
    "    G = nx.Graph(); num_colors = len(colors);\n",
    "    if num_colors == 0: raise ValueError(\"Color list empty.\");\n",
    "    if not 0.0 <= noise_level <= 1.0: raise ValueError(\"Noise level invalid.\")\n",
    "    nodes_per_region = num_nodes // num_colors; region_rng = random.Random(structure_seed); node_assignments = {}\n",
    "    nodes_list = list(range(1, num_nodes + 1)); region_rng.shuffle(nodes_list)\n",
    "    print(f\"  Assigning {num_nodes} nodes to {num_colors} color regions (Noise: {noise_level*100:.1f}%)...\")\n",
    "    for i, node_id in enumerate(nodes_list):\n",
    "        region_index = min(i // nodes_per_region, num_colors - 1); base_color = colors[region_index]; final_color = base_color\n",
    "        if noise_level > 0 and region_rng.random() < noise_level:\n",
    "            possible_noise_colors = [c for c in colors if c != base_color]\n",
    "            if possible_noise_colors: final_color = region_rng.choice(possible_noise_colors)\n",
    "        node_assignments[node_id] = final_color\n",
    "    shape_rng = random.Random(structure_seed + 1); shapes = ['circle', 'square', 'triangle']; shininess_options = ['shiny', 'not_shiny']\n",
    "    for i in range(1, num_nodes + 1):\n",
    "          shape = shape_rng.choice(shapes); shininess = shape_rng.choice(shininess_options)\n",
    "          G.add_node(i, color=node_assignments[i], shape=shape, shiny=shininess)\n",
    "    edge_rng = random.Random(edge_seed); edge_count = 0\n",
    "    for i in range(1, num_nodes + 1):\n",
    "        for j in range(i + 1, num_nodes + 1):\n",
    "            if edge_rng.random() < edge_probability: G.add_edge(i, j); edge_count += 1\n",
    "    if num_nodes > 0 and not nx.is_connected(G): warnings.warn(f\"Generated graph (seed {structure_seed}) not connected.\", stacklevel=2)\n",
    "    return G\n",
    "\n",
    "# ==============================================================================\n",
    "# Simulation (Unchanged)\n",
    "# ==============================================================================\n",
    "def simulate_agents(graph: nx.Graph, num_agents: int, num_traversals_per_agent: int, simulation_seed: int, edge_probability: float, target_preferred_count: int, max_steps: int, colors_list: List[str] = ['blue', 'green', 'red', 'yellow']) -> pd.DataFrame:\n",
    "    agent_data = []; node_id_to_color_name = {node: graph.nodes[node].get('color', 'unknown') for node in graph.nodes()}\n",
    "    sim_rng = random.Random(simulation_seed); print(f\" Starting simulations for Seed {simulation_seed}...\")\n",
    "    total_simulations = num_agents * num_traversals_per_agent; completed_simulations = 0; skipped_simulations = 0\n",
    "    print(f\"  NOTE: Agent goal: Find {target_preferred_count} preferred nodes (max {max_steps} steps).\"); print(\"  NOTE: Agent using DETERMINISTIC PREFERENCE strategy.\");\n",
    "    nodes_list = list(graph.nodes())\n",
    "    if not nodes_list: print(\" Error: Graph has no nodes.\"); return pd.DataFrame(agent_data)\n",
    "    for i in range(total_simulations):\n",
    "        agent_id = i % num_agents; start_node = sim_rng.choice(nodes_list); color_preference = sim_rng.choice(colors_list)\n",
    "        agent = Agent(start_node, color_preference, target_preferred_count, max_steps)\n",
    "        try: path, preferred_nodes_found, steps_taken = agent.traverse_graph(graph, sim_rng); # Call fixed Agent V5\n",
    "        except Exception as e: print(f\" Error during agent traversal call (Start: {start_node}, Pref: {color_preference}): {repr(e)}\"); skipped_simulations += 1; continue\n",
    "        if not path: print(f\"  Warning: Traversal returned empty path (Start: {start_node}). Skipping.\"); skipped_simulations += 1; continue\n",
    "        mapped_colors = [node_id_to_color_name.get(node, 'unknown') for node in agent.path]; path_len = len(agent.path); preferred_color_path_count = sum(1 for node in agent.path if graph.nodes.get(node, {}).get('color') == agent.color_preference); pref_prop = (preferred_color_path_count / path_len) if path_len > 0 else 0.0\n",
    "        agent_data.append({'Seed': simulation_seed, 'Agent': agent_id, 'Start Node': start_node, 'Color Preference': color_preference, 'Target Preferred Count': target_preferred_count, 'Max Steps': max_steps, 'Actual Steps Taken': steps_taken, 'Preferred Nodes Found': preferred_nodes_found, 'Path Length': path_len, 'Preferred Color Path Count': preferred_color_path_count, 'Preferred_Color_Proportion': pref_prop, 'Path': agent.path, 'Mapped Colors': mapped_colors, 'Visited Shapes': agent.visited_shapes, 'Visited Shininess': agent.visited_shininess, 'Density': edge_probability }); completed_simulations += 1\n",
    "    print(f\"  Finished simulations for Seed {simulation_seed}. Completed: {completed_simulations}, Skipped: {skipped_simulations}\")\n",
    "    return pd.DataFrame(agent_data)\n",
    "\n",
    "# ==============================================================================\n",
    "# Clustering Feature Engineering / Distance Calculation (UPDATED)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Jaccard (Unchanged) ---\n",
    "def jaccard_distance(set1: Any, set2: Any) -> float:\n",
    "    try: set1 = set(set1) if isinstance(set1, (list, tuple, np.ndarray)) else set(set1) if set1 is not None else set(); set2 = set(set2) if isinstance(set2, (list, tuple, np.ndarray)) else set(set2) if set2 is not None else set()\n",
    "    except TypeError: return 1.0\n",
    "    intersection = len(set1.intersection(set2)); union = len(set1.union(set2));\n",
    "    if union == 0: return 0.0\n",
    "    return 1.0 - intersection / union\n",
    "\n",
    "# --- BoC (Unchanged, but less likely to be used now) ---\n",
    "def extract_boc_features(group: pd.DataFrame) -> Optional[Any]:\n",
    "     if 'Mapped Colors' not in group.columns: print(\"Error:'Mapped Colors'\"); return None\n",
    "     group['Mapped Colors'] = group['Mapped Colors'].apply(lambda x: x if isinstance(x, list) else []) # Ensure list\n",
    "     corpus = group['Mapped Colors'].apply(lambda colors: ' '.join(map(str, colors)))\n",
    "     try: vectorizer = CountVectorizer(); X = vectorizer.fit_transform(corpus); print(f\"  BoC Features extracted with shape: {X.shape}\"); return X\n",
    "     except Exception as e: print(f\"  Error extracting BoC features: {e}\"); return None\n",
    "\n",
    "# --- Combined BoC + Numerical (Unchanged, but less likely to be used) ---\n",
    "def extract_combined_features(group: pd.DataFrame) -> Optional[Any]:\n",
    "    print(\"  Extracting Combined Features (BoC + Numerical)...\"); boc_features = extract_boc_features(group);\n",
    "    if boc_features is None: return None\n",
    "    num_cols = ['Path Length', 'Preferred_Color_Proportion'] # Ensure these cols exist\n",
    "    if not all(col in group.columns for col in num_cols): print(f\"Error: Missing {num_cols}\"); return None\n",
    "    try: numerical_features = group[num_cols].values.astype(float)\n",
    "    except Exception as e: print(f\"Error accessing numerical features: {e}\"); return None\n",
    "    try: scaler = StandardScaler(); scaled_numerical = scaler.fit_transform(numerical_features)\n",
    "    except Exception as e: print(f\"Error scaling numerical features: {e}\"); return None\n",
    "    try: combined_features = sparse_hstack((boc_features.tocsr(), scaled_numerical), format='csr'); print(f\"  Combined Features shape: {combined_features.shape}\"); return combined_features\n",
    "    except Exception as e: print(f\"Error combining features: {e}\"); return None\n",
    "\n",
    "# --- <<< NEW: Deep Learning Sequence Feature Extraction >>> ---\n",
    "def extract_sequence_features_dl(group: pd.DataFrame,\n",
    "                                 color_vocab: List[str],\n",
    "                                 max_seq_len: int,\n",
    "                                 embedding_dim: int,\n",
    "                                 lstm_units: int,\n",
    "                                 seed: int = 42) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts features from color sequences using an Embedding + LSTM model.\n",
    "    \"\"\"\n",
    "    print(f\"  Extracting Sequence Features using DL (MaxLen={max_seq_len}, Embed={embedding_dim}, LSTM={lstm_units})...\")\n",
    "    if 'Mapped Colors' not in group.columns:\n",
    "        print(\"  Error: 'Mapped Colors' column not found in group.\")\n",
    "        return None\n",
    "\n",
    "    # 1. Preprocessing\n",
    "    # Create color -> integer mapping (reserve 0 for padding)\n",
    "    color_to_int = {color: i + 1 for i, color in enumerate(color_vocab)}\n",
    "    vocab_size = len(color_vocab) + 1 # Add 1 for the padding token\n",
    "\n",
    "    # Convert color sequences to integer sequences\n",
    "    sequences = group['Mapped Colors'].apply(lambda x: [color_to_int.get(c, 0) for c in x] if isinstance(x, list) else [0])\n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_len, padding='post', truncating='post', value=0)\n",
    "    print(f\"   - Padded sequences shape: {padded_sequences.shape}\")\n",
    "\n",
    "    if padded_sequences.shape[0] == 0:\n",
    "        print(\"  Error: No valid sequences after padding.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Model Definition\n",
    "    # Ensure reproducibility within this function call if needed\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Use Input layer to handle potential variable input length if not using max_seq_len strictly\n",
    "    # Using input_length in Embedding is also fine since we pad\n",
    "    model = Sequential([\n",
    "        Input(shape=(max_seq_len,), dtype='int32'),\n",
    "        # Masking layer is crucial if using variable length sequences,\n",
    "        # but Embedding(mask_zero=True) does this implicitly for value 0\n",
    "        # Masking(mask_value=0.), # Alternative/Explicit way\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  input_length=max_seq_len, # Necessary if first layer\n",
    "                  mask_zero=True), # Important: Ignores padding in subsequent layers\n",
    "        # Use LSTM or GRU. GRU is slightly simpler/faster often.\n",
    "        # return_sequences=False to get only the last output state\n",
    "        LSTM(units=lstm_units, return_sequences=False),\n",
    "        # Optional: Add Dense layers for further transformation\n",
    "        # Dense(units=lstm_units // 2, activation='relu'),\n",
    "        # Dense(units=feature_output_dim) # If specific output size needed\n",
    "    ], name=f\"SequenceFeatureExtractor_Seed{seed}\")\n",
    "\n",
    "    # Compile is not strictly needed for prediction only, but good practice\n",
    "    # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') # Loss irrelevant here\n",
    "    # print(model.summary()) # Optional: Print model structure\n",
    "\n",
    "    # 3. Feature Extraction\n",
    "    try:\n",
    "        print(\"   - Predicting features with DL model...\")\n",
    "        # Ensure input is suitable type (numpy array)\n",
    "        features = model.predict(padded_sequences, verbose=0)\n",
    "        print(f\"   - Extracted DL Features shape: {features.shape}\")\n",
    "        if not np.isfinite(features).all():\n",
    "            warnings.warn(f\"NaN or Inf found in extracted DL features (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "            features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during DL model prediction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed error\n",
    "        return None\n",
    "# --- <<< END NEW DL FEATURE FUNCTION >>> ---\n",
    "\n",
    "\n",
    "# --- calculate_distance_matrix (MODIFIED to handle new method) ---\n",
    "def calculate_distance_matrix(group: pd.DataFrame,\n",
    "                               method: str = 'jaccard',\n",
    "                               # DL specific params (passed via kwargs)\n",
    "                               color_vocab: Optional[List[str]] = None,\n",
    "                               max_seq_len: Optional[int] = None,\n",
    "                               embedding_dim: Optional[int] = None,\n",
    "                               lstm_units: Optional[int] = None,\n",
    "                               seed: int = 42,\n",
    "                               **kwargs) -> Tuple[Optional[np.ndarray], Optional[Any], bool, str]:\n",
    "    \"\"\"\n",
    "    Calculates distance matrix or extracts features based on the specified method.\n",
    "    Now includes 'sequence_dl' method.\n",
    "    \"\"\"\n",
    "    num_samples = len(group); metric_used = method; distance_matrix = None; feature_matrix = None; is_valid = False\n",
    "\n",
    "    if method == 'jaccard':\n",
    "        # --- Jaccard calculation (unchanged) ---\n",
    "        metric_used = 'jaccard';\n",
    "        if 'Path' not in group.columns: print(\"Error:'Path'\"); return None, None, False, metric_used\n",
    "        group['Path_Set'] = group['Path'].apply(lambda x: set(x) if isinstance(x, (list, tuple)) else set())\n",
    "        distance_matrix_calc = np.zeros((num_samples, num_samples)); path_sets = group['Path_Set'].tolist()\n",
    "        for i in range(num_samples):\n",
    "            for j in range(i + 1, num_samples):\n",
    "                try: dist = jaccard_distance(path_sets[i], path_sets[j]);\n",
    "                except Exception as e: print(f\"Error Jaccard({i},{j}): {e}\"); return None, None, False, metric_used\n",
    "                if not np.isfinite(dist): raise ValueError(f\"Invalid Jaccard dist: {dist}\")\n",
    "                distance_matrix_calc[i, j] = dist; distance_matrix_calc[j, i] = dist\n",
    "        distance_matrix = distance_matrix_calc; is_valid = True\n",
    "    elif method == 'boc':\n",
    "        # --- BoC calculation (unchanged) ---\n",
    "        metric_used = kwargs.get('metric', 'cosine'); feature_matrix = extract_boc_features(group)\n",
    "        if feature_matrix is not None:\n",
    "            try: condensed_distances = pdist(feature_matrix.toarray(), metric=metric_used); distance_matrix = squareform(condensed_distances); print(f\"  BoC Features + '{metric_used}' distance matrix calculated.\"); is_valid = True\n",
    "            except Exception as e: print(f\"Warn: Could not create distance matrix from BoC: {e}\"); is_valid = False # Changed to False on error\n",
    "        else: return None, None, False, 'boc'\n",
    "    elif method == 'combined':\n",
    "        # --- Combined features (unchanged) ---\n",
    "         metric_used = kwargs.get('metric', 'euclidean'); feature_matrix = extract_combined_features(group)\n",
    "         if feature_matrix is not None: is_valid = True; distance_matrix = None; print(f\"  Combined features extracted. Assoc metric: '{metric_used}'\")\n",
    "         else: is_valid = False\n",
    "         # Return features directly for combined method\n",
    "         return distance_matrix, feature_matrix, is_valid, metric_used\n",
    "\n",
    "    # --- <<< NEW: Handle 'sequence_dl' method >>> ---\n",
    "    elif method == 'sequence_dl':\n",
    "        metric_used = kwargs.get('metric', 'euclidean') # Metric for evaluating clusters based on these features\n",
    "        if not all([color_vocab, max_seq_len, embedding_dim, lstm_units]):\n",
    "             print(\"Error: Missing DL parameters (color_vocab, max_seq_len, embedding_dim, lstm_units) for 'sequence_dl'.\")\n",
    "             return None, None, False, 'sequence_dl'\n",
    "\n",
    "        feature_matrix = extract_sequence_features_dl(group, color_vocab, max_seq_len, embedding_dim, lstm_units, seed=seed)\n",
    "\n",
    "        if feature_matrix is not None:\n",
    "            print(f\"  Sequence DL features extracted. Associated metric for clustering: '{metric_used}'\")\n",
    "            is_valid = True\n",
    "            distance_matrix = None # Features extracted, not distances directly\n",
    "        else:\n",
    "            print(\"Error: Failed to extract sequence DL features.\")\n",
    "            is_valid = False\n",
    "        # Return features for DL method\n",
    "        return distance_matrix, feature_matrix, is_valid, metric_used\n",
    "    # --- <<< END NEW DL METHOD HANDLING >>> ---\n",
    "\n",
    "    else: print(f\"Error: Unknown method '{method}'.\"); return None, None, False, 'unknown'\n",
    "\n",
    "    # Return distance matrix (for jaccard/boc) or feature matrix (for combined/dl)\n",
    "    return distance_matrix, feature_matrix, is_valid, metric_used\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Clustering and Evaluation Function (MODIFIED to use new features)\n",
    "# ==============================================================================\n",
    "def cluster_and_evaluate(df: pd.DataFrame,\n",
    "                         n_clusters: int = 4,\n",
    "                         feature_distance_method: str = 'jaccard',\n",
    "                         cluster_method: str = 'ward',\n",
    "                         dist_metric_param: str = 'cosine', # Metric for BoC distances or DL feature eval\n",
    "                         linkage_method: str = 'ward', # Used by hierarchical\n",
    "                         # DL specific params\n",
    "                         color_vocab: Optional[List[str]] = None,\n",
    "                         max_seq_len: Optional[int] = None,\n",
    "                         embedding_dim: Optional[int] = None,\n",
    "                         lstm_units: Optional[int] = None,\n",
    "                         ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Performs clustering and evaluation, now supporting 'sequence_dl' features.\n",
    "    \"\"\"\n",
    "    required_cols = ['Seed', 'Color Preference', 'Mapped Colors'] # Base required\n",
    "    # Adjust required cols check based on feature method\n",
    "    if feature_distance_method == 'jaccard': required_cols.append('Path')\n",
    "    elif feature_distance_method == 'combined': required_cols.extend(['Path Length', 'Preferred Color Path Count', 'Path', 'Mapped Colors'])\n",
    "    elif feature_distance_method == 'boc': required_cols.extend(['Path', 'Mapped Colors'])\n",
    "    elif feature_distance_method == 'sequence_dl': required_cols.append('Mapped Colors') # Only need sequences\n",
    "\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Missing columns for eval: {missing}. Available: {df.columns.tolist()}\")\n",
    "\n",
    "    print(f\"\\n--- Starting Clustering Evaluation ---\");\n",
    "    print(f\" Feature/Distance Method: {feature_distance_method}\");\n",
    "    print(f\" Clustering Algorithm: {cluster_method}\");\n",
    "    print(f\" Target Clusters (k): {n_clusters}\")\n",
    "    if feature_distance_method == 'sequence_dl':\n",
    "        print(f\" DL Params: MaxLen={max_seq_len}, Embed={embedding_dim}, LSTM={lstm_units}\")\n",
    "\n",
    "\n",
    "    results_summary = []; processed_groups = []\n",
    "    for seed, group in df.groupby('Seed'):\n",
    "        print(f\" Processing Seed {seed}...\"); group = group.copy(); num_samples = len(group); group['Cluster'] = -1\n",
    "        if num_samples < 2: print(f\"  Skipping Seed {seed}: Samples < 2.\"); processed_groups.append(group); continue\n",
    "\n",
    "        # --- Call calculate_distance_matrix (handles all methods) ---\n",
    "        distance_matrix, feature_matrix, is_valid, eval_metric = calculate_distance_matrix(\n",
    "            group,\n",
    "            method=feature_distance_method,\n",
    "            metric=dist_metric_param, # Passed as kwarg\n",
    "            # Pass DL params if method is 'sequence_dl'\n",
    "            color_vocab=color_vocab,\n",
    "            max_seq_len=max_seq_len,\n",
    "            embedding_dim=embedding_dim,\n",
    "            lstm_units=lstm_units,\n",
    "            seed=seed # Pass seed for reproducibility in DL feature extraction\n",
    "        )\n",
    "\n",
    "        if not is_valid:\n",
    "            print(f\"  Skipping Seed {seed}: Feature/Distance calculation failed or invalid.\")\n",
    "            processed_groups.append(group); continue\n",
    "\n",
    "        # Determine if we primarily have features or distances\n",
    "        # DL method *always* returns features.\n",
    "        # Combined method *always* returns features.\n",
    "        # Jaccard *always* returns distances.\n",
    "        # BoC *can* return features or distances, but we usually use features now.\n",
    "        has_features = feature_matrix is not None\n",
    "        has_distances = distance_matrix is not None\n",
    "\n",
    "        clusters = None; actual_n_clusters = min(n_clusters, num_samples) if num_samples > 0 else 1;\n",
    "        if actual_n_clusters < 1: actual_n_clusters = 1\n",
    "\n",
    "        # --- Clustering Block ---\n",
    "        try:\n",
    "            if cluster_method in ['ward', 'complete', 'average', 'single']: # Hierarchical\n",
    "                 print(f\"  Running Hierarchical Clustering ({cluster_method})...\")\n",
    "                 linkage_input_dm = distance_matrix\n",
    "                 if linkage_input_dm is None: # Need to calculate distances from features\n",
    "                     if has_features:\n",
    "                         print(f\"   - Calculating '{dist_metric_param}' distance matrix for hierarchical on extracted features...\")\n",
    "                         try:\n",
    "                             # Use .toarray() if features are sparse (e.g., from BoC/Combined)\n",
    "                             feature_array = feature_matrix.toarray() if hasattr(feature_matrix, \"toarray\") else feature_matrix\n",
    "                             condensed_distances_hier = pdist(feature_array, metric=dist_metric_param)\n",
    "                             linkage_input_dm = squareform(condensed_distances_hier)\n",
    "                         except Exception as e_pdist: raise ValueError(f\"Failed distance calc for hierarchical: {e_pdist}\") from e_pdist\n",
    "                     else: raise ValueError(\"Hierarchical clustering selected, but neither distance matrix nor features are available.\")\n",
    "\n",
    "                 # --- Validation checks for distance matrix (mostly unchanged) ---\n",
    "                 if not isinstance(linkage_input_dm, np.ndarray) or linkage_input_dm.ndim!=2 or linkage_input_dm.shape[0]!=linkage_input_dm.shape[1]: raise ValueError(\"Invalid DM shape.\")\n",
    "                 if np.isnan(linkage_input_dm).any() or np.isinf(linkage_input_dm).any(): raise ValueError(\"NaN/Inf in DM.\")\n",
    "                 if not np.allclose(linkage_input_dm, linkage_input_dm.T): warnings.warn(f\"DM not symmetric (Seed {seed}). Forcing symmetry.\", stacklevel=2); linkage_input_dm = (linkage_input_dm + linkage_input_dm.T) / 2\n",
    "                 np.fill_diagonal(linkage_input_dm, 0)\n",
    "                 linkage_input_dm[linkage_input_dm < 0] = 0 # Ensure non-negative distances\n",
    "\n",
    "                 try: condensed_distance = squareform(linkage_input_dm, checks=True)\n",
    "                 except ValueError as sq_err: raise ValueError(f\"Squareform check failed: {sq_err}\") from sq_err\n",
    "                 if condensed_distance is None or not np.isfinite(condensed_distance).all(): raise ValueError(\"Invalid condensed distance array.\")\n",
    "\n",
    "                 Z = linkage(condensed_distance, method=cluster_method);\n",
    "                 if Z is None or not isinstance(Z, np.ndarray): raise TypeError(\"Linkage failed.\")\n",
    "                 clusters = fcluster(Z, t=actual_n_clusters, criterion='maxclust')\n",
    "                 if clusters is None or not isinstance(clusters, np.ndarray): raise TypeError(\"fcluster failed.\")\n",
    "                 clusters = clusters # Already 1-based\n",
    "\n",
    "            elif cluster_method in ['kmeans', 'gmm']: # Feature-based\n",
    "                print(f\"  Running {cluster_method} Clustering...\")\n",
    "                if not has_features:\n",
    "                    raise ValueError(f\"{cluster_method} requires features, but they were not generated by method '{feature_distance_method}'.\")\n",
    "\n",
    "                # Ensure features are dense numpy array for KMeans/GMM\n",
    "                feature_array = feature_matrix.toarray() if hasattr(feature_matrix, \"toarray\") else feature_matrix\n",
    "\n",
    "                if cluster_method == 'kmeans':\n",
    "                     # Handle potential NaN/Inf in features before KMeans\n",
    "                     if not np.isfinite(feature_array).all():\n",
    "                         warnings.warn(f\"NaN/Inf found in features for KMeans (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "                         feature_array = np.nan_to_num(feature_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                     if feature_array.shape[0] < actual_n_clusters:\n",
    "                         warnings.warn(f\"Samples ({feature_array.shape[0]}) < clusters ({actual_n_clusters}) for KMeans (Seed {seed}). Reducing clusters.\", stacklevel=2)\n",
    "                         actual_n_clusters = max(1, feature_array.shape[0]) # Ensure at least 1 cluster\n",
    "                     kmeans = KMeans(n_clusters=actual_n_clusters, random_state=seed, n_init=10, verbose=0);\n",
    "                     clusters = kmeans.fit_predict(feature_array);\n",
    "                     clusters += 1 # Make 1-based\n",
    "                elif cluster_method == 'gmm':\n",
    "                     # Handle potential NaN/Inf in features before GMM\n",
    "                     if not np.isfinite(feature_array).all():\n",
    "                         warnings.warn(f\"NaN/Inf found in features for GMM (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "                         feature_array = np.nan_to_num(feature_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                     if feature_array.shape[0] < actual_n_clusters:\n",
    "                         warnings.warn(f\"Samples ({feature_array.shape[0]}) < components ({actual_n_clusters}) for GMM (Seed {seed}). Reducing components.\", stacklevel=2)\n",
    "                         actual_n_clusters = max(1, feature_array.shape[0]) # Ensure at least 1 component\n",
    "                     gmm = GaussianMixture(n_components=actual_n_clusters, random_state=seed, verbose=0, n_init=5);\n",
    "                     clusters = gmm.fit_predict(feature_array);\n",
    "                     clusters += 1 # Make 1-based\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported cluster_method: {cluster_method}\")\n",
    "\n",
    "            if clusters is None:\n",
    "                 raise ValueError(\"Clustering did not produce cluster assignments.\")\n",
    "            if len(clusters) != num_samples:\n",
    "                 raise ValueError(f\"Number of cluster labels ({len(clusters)}) does not match number of samples ({num_samples}).\")\n",
    "\n",
    "            group['Cluster'] = clusters\n",
    "            num_unique_clusters_found = len(set(c for c in clusters if c != -1)) # Assuming -1 is noise/unassigned\n",
    "            print(f\"   - Found {num_unique_clusters_found} unique clusters.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error during clustering for Seed {seed}: {repr(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            processed_groups.append(group); continue # Skip evaluation if clustering fails\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        silhouette_avg = np.nan; ari_score = np.nan; nmi_score = np.nan\n",
    "        valid_cluster_mask = group['Cluster'] != -1 # Exclude potential noise points (e.g., from DBSCAN if used)\n",
    "        clusters_for_eval = group.loc[valid_cluster_mask, 'Cluster']\n",
    "        num_valid_samples_for_eval = len(clusters_for_eval);\n",
    "        num_clusters_for_eval = len(set(clusters_for_eval))\n",
    "\n",
    "        # --- Silhouette Score Calculation ---\n",
    "        if num_clusters_for_eval > 1 and num_clusters_for_eval < num_valid_samples_for_eval:\n",
    "            print(f\"   - Calculating Silhouette Score (using metric: '{eval_metric}')...\")\n",
    "            silhouette_input_data = None\n",
    "            silhouette_metric_param = 'euclidean' # Default\n",
    "\n",
    "            if has_distances and eval_metric == 'jaccard': # Use precomputed Jaccard\n",
    "                 silhouette_metric_param = 'precomputed'\n",
    "                 silhouette_input_data = distance_matrix\n",
    "                 print(\"     (Using precomputed Jaccard distance matrix)\")\n",
    "            elif has_features: # Use features with the specified eval_metric\n",
    "                 silhouette_metric_param = eval_metric # Use the metric associated with features (e.g., euclidean for DL)\n",
    "                 silhouette_input_data = feature_matrix\n",
    "                 print(f\"     (Using extracted features)\")\n",
    "            else: # Fallback: calculate distances if only precomputed available but not Jaccard\n",
    "                 if has_distances:\n",
    "                     silhouette_metric_param = 'precomputed'\n",
    "                     silhouette_input_data = distance_matrix\n",
    "                     print(f\"     (Using provided precomputed distance matrix - metric assumed {eval_metric})\")\n",
    "\n",
    "\n",
    "            if silhouette_input_data is not None:\n",
    "                try:\n",
    "                    # Get the subset of data corresponding to valid clusters\n",
    "                    valid_mask_np = valid_cluster_mask.to_numpy()\n",
    "                    if silhouette_metric_param == 'precomputed':\n",
    "                        if silhouette_input_data.shape[0] == num_samples:\n",
    "                            valid_distance_matrix = silhouette_input_data[np.ix_(valid_mask_np, valid_mask_np)]\n",
    "                            if valid_distance_matrix.shape[0] > 1: # Need at least 2 samples in valid clusters\n",
    "                                silhouette_avg = silhouette_score(valid_distance_matrix, clusters_for_eval, metric='precomputed')\n",
    "                            else: print(\"     Skipping Silhouette: Not enough valid samples after filtering for precomputed.\")\n",
    "                        else: print(f\"     Warning: Precomputed matrix shape mismatch ({silhouette_input_data.shape[0]} vs {num_samples}) for Silhouette.\")\n",
    "                    else: # Feature-based silhouette\n",
    "                        feature_array_eval = silhouette_input_data.toarray() if hasattr(silhouette_input_data, \"toarray\") else silhouette_input_data\n",
    "                        if feature_array_eval.shape[0] == num_samples:\n",
    "                            valid_feature_matrix = feature_array_eval[valid_mask_np]\n",
    "                            if valid_feature_matrix.shape[0] > 1:\n",
    "                                # Handle potential NaN/Inf in features before Silhouette\n",
    "                                if not np.isfinite(valid_feature_matrix).all():\n",
    "                                     warnings.warn(f\"NaN/Inf found in features for Silhouette (Seed {seed}). Replacing with 0.\", stacklevel=2)\n",
    "                                     valid_feature_matrix = np.nan_to_num(valid_feature_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                                silhouette_avg = silhouette_score(valid_feature_matrix, clusters_for_eval, metric=silhouette_metric_param)\n",
    "                            else: print(\"     Skipping Silhouette: Not enough valid samples after filtering for feature-based.\")\n",
    "                        else: print(f\"     Warning: Feature matrix shape mismatch ({feature_array_eval.shape[0]} vs {num_samples}) for Silhouette.\")\n",
    "                    print(f\"     Silhouette Score: {silhouette_avg:.4f}\")\n",
    "                except Exception as e_sil:\n",
    "                    print(f\"    Warning: Silhouette score calculation error (Metric: {silhouette_metric_param}): {repr(e_sil)}\")\n",
    "            else:\n",
    "                 print(f\"    Skipping Silhouette: Suitable input data not available (Seed {seed})\")\n",
    "        elif num_clusters_for_eval <= 1:\n",
    "             print(f\"   - Skipping Silhouette Score: Only {num_clusters_for_eval} cluster(s) found.\")\n",
    "        else: # Not enough samples\n",
    "             print(f\"   - Skipping Silhouette Score: Not enough samples ({num_valid_samples_for_eval}) for {num_clusters_for_eval} clusters.\")\n",
    "\n",
    "\n",
    "        # --- ARI/NMI Calculation (unchanged) ---\n",
    "        if num_valid_samples_for_eval > 0:\n",
    "            print(\"   - Calculating ARI and NMI...\")\n",
    "            try:\n",
    "                true_labels_for_eval = group.loc[valid_cluster_mask, 'Color Preference'];\n",
    "                ari_score = adjusted_rand_score(true_labels_for_eval, clusters_for_eval);\n",
    "                nmi_score = normalized_mutual_info_score(true_labels_for_eval, clusters_for_eval)\n",
    "                print(f\"     ARI: {ari_score:.4f}, NMI: {nmi_score:.4f}\")\n",
    "            except Exception as e_gnd:\n",
    "                 print(f\"    Warning: Could not compute ARI/NMI Seed {seed}: {repr(e_gnd)}\")\n",
    "\n",
    "        # --- Purity/Color Percentage Calculation (unchanged) ---\n",
    "        formatted_cluster_color_percentages = {}; max_color_separation = np.nan\n",
    "        try:\n",
    "            valid_group_for_purity = group[group['Cluster'] != -1]\n",
    "            if not valid_group_for_purity.empty:\n",
    "                if 'Mapped Colors' in valid_group_for_purity.columns:\n",
    "                     # ... (rest of purity calculation code is identical) ...\n",
    "                     exploded_group = valid_group_for_purity.explode('Mapped Colors').dropna(subset=['Mapped Colors']); exploded_group.rename(columns={'Mapped Colors': 'Flat Colors'}, inplace=True)\n",
    "                     color_counts = exploded_group.groupby(['Cluster', 'Flat Colors']).size().unstack(fill_value=0)\n",
    "                     all_possible_colors = sorted(df['Color Preference'].dropna().unique())\n",
    "                     for color in all_possible_colors:\n",
    "                         if color not in color_counts.columns: color_counts[color] = 0\n",
    "                     color_counts = color_counts[all_possible_colors]; cluster_sums = color_counts.sum(axis=1); safe_sums = cluster_sums.replace(0, 1)\n",
    "                     color_percentages = color_counts.div(safe_sums, axis=0).mul(100); cluster_color_percentages_dict = color_percentages.round(2).apply(lambda r: r.dropna().to_dict(), axis=1).to_dict()\n",
    "                     formatted_cluster_color_percentages = {int(k): {c: f\"{p:.2f}%\" for c, p in v.items()} for k, v in cluster_color_percentages_dict.items()}\n",
    "                     if not color_percentages.empty: max_color_separation = color_percentages.apply(lambda r: r.max() - r.min() if not r.empty else 0.0, axis=1).max()\n",
    "                     else: max_color_separation = 0.0\n",
    "                else: print(f\"    Warning: 'Mapped Colors' column missing for purity calc (Seed {seed}).\")\n",
    "        except Exception as e: print(f\"    Warning: Color percentage error Seed {seed}: {e}\")\n",
    "\n",
    "        results_summary.append({\n",
    "            'Seed': seed,\n",
    "            'Algorithm': cluster_method,\n",
    "            'Feature_Metric': f\"{feature_distance_method}({eval_metric})\",\n",
    "            'Num_Clusters_Target': n_clusters,\n",
    "            'Num_Clusters_Found': num_clusters_for_eval,\n",
    "            'Cluster_Color_Percentage': formatted_cluster_color_percentages,\n",
    "            'Silhouette': silhouette_avg,\n",
    "            'ARI': ari_score,\n",
    "            'NMI': nmi_score,\n",
    "            'Max_Color_Separation': max_color_separation\n",
    "        })\n",
    "        processed_groups.append(group) # Add group with cluster assignments\n",
    "\n",
    "    # Combine results\n",
    "    if not processed_groups: warnings.warn(\"No groups processed.\"); return pd.DataFrame(results_summary), df.copy()\n",
    "    df_with_clusters = pd.concat(processed_groups).reset_index(drop=True);\n",
    "    results_summary_df = pd.DataFrame(results_summary)\n",
    "    print(\"\\nClustering and evaluation complete.\")\n",
    "    return results_summary_df, df_with_clusters\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Plotting Functions (Unchanged)\n",
    "# ==============================================================================\n",
    "def plot_colored_graph(G: nx.Graph, seed: int, output_dir: str = \"plots\"):\n",
    "     if not os.path.exists(output_dir): os.makedirs(output_dir); filepath = os.path.join(output_dir, f\"graph_seed_{seed}.png\")\n",
    "     plt.figure(figsize=(12, 10)); pos = nx.spring_layout(G, seed=42); node_colors = [G.nodes[node].get('color', 'gray') for node in G.nodes()]\n",
    "     nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=200, alpha=0.9); nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
    "     plt.title(f\"Graph Structure (Seed {seed})\", fontsize=16); plt.axis('off'); plt.tight_layout(); plt.savefig(filepath); print(f\" Saved graph plot to {filepath}\"); plt.close()\n",
    "\n",
    "def plot_color_confusion_matrix(df: pd.DataFrame, cluster_col: str = 'Cluster', true_color_col: str = 'Color Preference', figsize: Tuple[int, int] = (8, 7), cmap: str = 'Blues', output_dir: str = \"plots\"):\n",
    "     if not os.path.exists(output_dir): os.makedirs(output_dir); plot_filename = f\"confusion_matrix_{cluster_col}_vs_{true_color_col}.png\"; filepath = os.path.join(output_dir, plot_filename)\n",
    "     if cluster_col not in df.columns: raise ValueError(f\"Cluster column '{cluster_col}' not found.\")\n",
    "     if true_color_col not in df.columns: raise ValueError(f\"True color column '{true_color_col}' not found.\")\n",
    "     # Ensure cluster column is numeric, handle potential non-numeric values\n",
    "     df[cluster_col] = pd.to_numeric(df[cluster_col], errors='coerce')\n",
    "     df_valid = df.dropna(subset=[cluster_col]).copy() # Drop rows where cluster is NaN after coercion\n",
    "     df_valid[cluster_col] = df_valid[cluster_col].astype(int);\n",
    "     # Filter based on meaningful cluster IDs (e.g., > 0 if -1 is noise)\n",
    "     df_valid = df_valid[df_valid[cluster_col] > 0] # Assuming clusters are 1-based\n",
    "\n",
    "     if df_valid.empty: print(f\"Warning: No valid data for confusion matrix (Cluster Col: {cluster_col}).\"); return\n",
    "\n",
    "     unique_colors = sorted(df_valid[true_color_col].dropna().unique());\n",
    "     if not unique_colors: print(f\"Error: No valid unique values in '{true_color_col}'.\"); return\n",
    "\n",
    "     # Use actual unique cluster labels found for columns if remapping isn't perfect\n",
    "     unique_pred_labels_found = sorted(df_valid[cluster_col].unique())\n",
    "     if not unique_pred_labels_found:\n",
    "         print(f\"Error: No valid unique predicted cluster labels found in '{cluster_col}'.\"); return\n",
    "\n",
    "     # Map true colors to integers for rows\n",
    "     color_to_int_mapping = {color: i + 1 for i, color in enumerate(unique_colors)};\n",
    "     int_to_color_mapping = {v: k for k,v in color_to_int_mapping.items()}\n",
    "     row_labels_int = list(color_to_int_mapping.values())\n",
    "     row_labels_names = [int_to_color_mapping.get(i, f'Unknown {i}') for i in row_labels_int]\n",
    "\n",
    "\n",
    "     print(f\"\\n--- Confusion Matrix Generation ({cluster_col} vs {true_color_col}) ---\");\n",
    "     print(f\" Using True Color Mapping for Plot Rows: {color_to_int_mapping}\")\n",
    "     print(f\" Using Predicted Cluster Labels for Plot Columns: {unique_pred_labels_found}\")\n",
    "\n",
    "     try:\n",
    "         y_true = df_valid[true_color_col].map(color_to_int_mapping);\n",
    "         y_pred = df_valid[cluster_col] # Already filtered to be integers > 0\n",
    "     except Exception as e: print(f\" Error preparing y_true/y_pred: {e}\"); return\n",
    "\n",
    "     valid_indices = y_true.notna()\n",
    "     y_true = y_true[valid_indices].astype(int);\n",
    "     y_pred = y_pred[valid_indices]\n",
    "\n",
    "     if len(y_true) == 0: print(\" Error: No samples left for confusion matrix after filtering NAs.\"); return\n",
    "\n",
    "     # Ensure labels cover the range of observed true and predicted values\n",
    "     all_row_labels = sorted(list(set(row_labels_int).union(set(y_true.unique()))))\n",
    "     all_col_labels = sorted(list(set(unique_pred_labels_found).union(set(y_pred.unique()))))\n",
    "\n",
    "\n",
    "     conf_matrix = confusion_matrix(y_true, y_pred, labels=all_row_labels)\n",
    "     # Pad columns if necessary if prediction didn't yield all possible cluster numbers\n",
    "     if conf_matrix.shape[1] < len(all_col_labels):\n",
    "         padded_matrix = np.zeros((conf_matrix.shape[0], len(all_col_labels)), dtype=int)\n",
    "         # Find indices corresponding to existing columns\n",
    "         existing_cols_indices = [all_col_labels.index(label) for label in sorted(y_pred.unique()) if label in all_col_labels]\n",
    "         padded_matrix[:, existing_cols_indices] = conf_matrix\n",
    "         conf_matrix = padded_matrix\n",
    "\n",
    "     # Ensure consistent row/col names for the DataFrame\n",
    "     row_names = [int_to_color_mapping.get(i, f'Err {i}') for i in all_row_labels]\n",
    "     col_names = [f\"Cluster {i}\" for i in all_col_labels] # Use generic \"Cluster X\" for columns\n",
    "\n",
    "     conf_matrix_df = pd.DataFrame(conf_matrix, index=row_names, columns=col_names)\n",
    "\n",
    "     plt.figure(figsize=figsize); sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap=cmap, cbar=True, linewidths=.5, linecolor='lightgray', annot_kws={\"size\": 10})\n",
    "     title = f'Confusion Matrix: Predicted Cluster ({cluster_col}) vs True Color ({true_color_col})'\n",
    "     if 'Remapped' in cluster_col: title += \" (Remapped)\"\n",
    "     plt.title(title, fontsize=14); plt.ylabel(f'True Color (`{true_color_col}`)', fontsize=12); plt.xlabel(f'Predicted Label (`{cluster_col}`)', fontsize=12)\n",
    "     plt.xticks(rotation=45, ha='right', fontsize=10); plt.yticks(rotation=0, fontsize=10)\n",
    "     plt.tight_layout(); plt.savefig(filepath); print(f\" Saved confusion matrix plot to {filepath}\"); plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block -> Using SEQUENCE_DL + KMEANS\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Parameters ---\n",
    "    NUM_NODES = 40; COLORS = ['blue', 'green', 'red', 'yellow', 'white']; EDGE_PROBABILITY = 0.20\n",
    "    NUM_SEEDS = 5 # Reduced for faster testing of DL\n",
    "    NUM_AGENTS = 5; NUM_TRAVERSALS_PER_AGENT = 20; # Reduced for faster testing\n",
    "\n",
    "    # --- Simulation Parameters ---\n",
    "    TARGET_PREFERRED_COUNT = 5 # Agent goal\n",
    "    MAX_STEPS = 30             # Step limit (Reduced slightly for potentially shorter sequences)\n",
    "    GRAPH_NOISE_LEVEL = 0.0\n",
    "\n",
    "    # --- <<< NEW: Deep Learning Parameters >>> ---\n",
    "    # Determine MAX_SEQ_LEN: Analyze typical path lengths or set a fixed cap\n",
    "    # MAX_SEQ_LEN = MAX_STEPS + 1 # A reasonable upper bound based on simulation limit + start node\n",
    "    # OR analyze data first if running simulation separately:\n",
    "    # path_lengths = all_simulation_data['Path Length']\n",
    "    # MAX_SEQ_LEN = int(path_lengths.quantile(0.95)) # e.g., 95th percentile\n",
    "    MAX_SEQ_LEN = 35 # Set fixed based on slightly above MAX_STEPS for now\n",
    "    EMBEDDING_DIM = 16 # Dimension of color embeddings\n",
    "    LSTM_UNITS = 32    # Number of units in the LSTM layer (size of feature vector)\n",
    "\n",
    "    # --- Clustering Set to SEQUENCE_DL + KMEANS ---\n",
    "    N_CLUSTERS = len(COLORS) # Match number of expected preference groups\n",
    "    FEATURE_DISTANCE_METHOD = 'sequence_dl' # <<< USE THE NEW METHOD >>>\n",
    "    CLUSTER_METHOD = 'kmeans'              # KMeans works directly on features\n",
    "    DIST_METRIC = 'euclidean'              # Metric used by KMeans and for Silhouette on features\n",
    "\n",
    "    # --- Output Directory ---\n",
    "    OUTPUT_DIR = f\"clustering_results_{FEATURE_DISTANCE_METHOD}_lstm{LSTM_UNITS}_embed{EMBEDDING_DIM}_k{N_CLUSTERS}\"\n",
    "    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # --- Run Simulation (Unchanged) ---\n",
    "    print(f\"--- Starting Simulation Phase ---\")\n",
    "    all_results_list = []; start_time_sim = time.time()\n",
    "    for seed in range(NUM_SEEDS):\n",
    "        graph_attribute_seed = seed; graph_edge_seed = seed + NUM_SEEDS\n",
    "        print(f\"\\nGenerating structured graph for Seed {seed}...\")\n",
    "        G = generate_structured_colored_graph(NUM_NODES, COLORS, EDGE_PROBABILITY,\n",
    "                                              graph_attribute_seed, graph_edge_seed,\n",
    "                                              noise_level=GRAPH_NOISE_LEVEL)\n",
    "        print(f\"Running Simulation for Seed {seed}...\")\n",
    "        simulation_seed = seed\n",
    "        df_agents = simulate_agents(G, NUM_AGENTS, NUM_TRAVERSALS_PER_AGENT,\n",
    "                                    simulation_seed, EDGE_PROBABILITY,\n",
    "                                    target_preferred_count=TARGET_PREFERRED_COUNT,\n",
    "                                    max_steps=MAX_STEPS,\n",
    "                                    colors_list=COLORS)\n",
    "        all_results_list.append(df_agents)\n",
    "    all_simulation_data = pd.concat(all_results_list, ignore_index=True); end_time_sim = time.time()\n",
    "    print(f\"\\n--- Simulation Phase Complete ---\"); print(f\"Generated {len(all_simulation_data)} records in {end_time_sim - start_time_sim:.2f}s.\")\n",
    "    print(f\"Path Length Stats:\\n{all_simulation_data['Path Length'].describe()}\")\n",
    "\n",
    "    # Check if simulation produced data\n",
    "    if all_simulation_data.empty:\n",
    "         print(\"\\nERROR: Simulation produced no data. Exiting.\")\n",
    "    else:\n",
    "        # --- Run Clustering and Evaluation ---\n",
    "        print(f\"\\n--- Starting Clustering Phase using {FEATURE_DISTANCE_METHOD} features ---\"); start_time_cluster = time.time()\n",
    "        summary_results, data_with_clusters = cluster_and_evaluate(\n",
    "            all_simulation_data.copy(),\n",
    "            n_clusters=N_CLUSTERS,\n",
    "            feature_distance_method=FEATURE_DISTANCE_METHOD,\n",
    "            cluster_method=CLUSTER_METHOD,\n",
    "            dist_metric_param=DIST_METRIC, # Pass the metric for Silhouette\n",
    "            # Pass DL parameters explicitly\n",
    "            color_vocab=COLORS,\n",
    "            max_seq_len=MAX_SEQ_LEN,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            lstm_units=LSTM_UNITS\n",
    "        )\n",
    "        end_time_cluster = time.time(); print(f\"\\n--- Clustering Phase Complete ---\"); print(f\"Completed in {end_time_cluster - start_time_cluster:.2f}s.\")\n",
    "\n",
    "        # --- Display and Save Results (Unchanged) ---\n",
    "        print(\"\\n--- Clustering Summary Results ---\"); pd.set_option('display.max_rows', 50); pd.set_option('display.max_columns', None); pd.set_option('display.width', 120); pd.set_option('display.max_colwidth', 150)\n",
    "        if not summary_results.empty:\n",
    "            print(summary_results.round(4).to_string()); avg_ari = summary_results['ARI'].mean(); avg_nmi = summary_results['NMI'].mean(); avg_silhouette = summary_results['Silhouette'].mean()\n",
    "            print(\"-\" * 50); print(f\"Avg ARI: {avg_ari:.4f}\"); print(f\"Avg NMI: {avg_nmi:.4f}\"); print(f\"Avg Silhouette: {avg_silhouette:.4f}\"); print(\"-\" * 50)\n",
    "            summary_filename = f\"summary_{FEATURE_DISTANCE_METHOD}_lstm{LSTM_UNITS}_k{N_CLUSTERS}.csv\"\n",
    "            summary_filepath = os.path.join(OUTPUT_DIR, summary_filename)\n",
    "            try: summary_results.to_csv(summary_filepath, index=False); print(f\"Summary results saved to {summary_filepath}\")\n",
    "            except Exception as e: print(f\"Error saving summary: {e}\")\n",
    "        else: print(\"No summary results generated.\")\n",
    "\n",
    "\n",
    "        # --- Generate and Save Confusion Matrix (with Optimal Remapping - Unchanged Logic) ---\n",
    "        if not data_with_clusters.empty and 'Cluster' in data_with_clusters.columns and data_with_clusters['Cluster'].max() > 0 :\n",
    "             print(\"\\n--- Remapping cluster labels for optimal visualization using Hungarian algorithm ---\")\n",
    "             try:\n",
    "                 valid_clusters_df = data_with_clusters[data_with_clusters['Cluster'] > 0].copy()\n",
    "                 if valid_clusters_df.empty: raise ValueError(\"No valid clusters found after filtering for remapping.\")\n",
    "\n",
    "                 unique_colors = sorted(data_with_clusters['Color Preference'].dropna().unique())\n",
    "                 true_color_to_plot_int = {color: i + 1 for i, color in enumerate(unique_colors)}\n",
    "                 plot_int_to_true_color = {v: k for k,v in true_color_to_plot_int.items()}\n",
    "                 plot_labels_ordered = [plot_int_to_true_color.get(i, f'Unknown {i}') for i in range(1, len(unique_colors) + 1)]\n",
    "                 print(f\"  Target Plot Order Mapping: {true_color_to_plot_int}\")\n",
    "\n",
    "                 y_true_numeric_target_order = valid_clusters_df['Color Preference'].map(true_color_to_plot_int)\n",
    "                 y_pred_original_cluster_labels = valid_clusters_df['Cluster']\n",
    "\n",
    "                 # Labels for confusion_matrix calculation need to encompass all possibilities up to k\n",
    "                 k = N_CLUSTERS\n",
    "                 matrix_calc_labels_true = list(range(1, k + 1)) # Expected true labels 1..k\n",
    "                 matrix_calc_labels_pred = sorted(y_pred_original_cluster_labels.unique()) # Actual predicted labels\n",
    "\n",
    "                 # Create matrix with consistent labels\n",
    "                 raw_cm = confusion_matrix(y_true_numeric_target_order.dropna(),\n",
    "                                           y_pred_original_cluster_labels.loc[y_true_numeric_target_order.notna()],\n",
    "                                           labels=matrix_calc_labels_true) # Rows correspond to true target ints\n",
    "\n",
    "                 # Adjust columns if prediction labels don't match 1..k exactly\n",
    "                 # This part might need refinement depending on how Hungarian algorithm handles non-square\n",
    "                 print(\"  Raw Confusion Matrix (Rows: True Target Int, Cols: Based on Observed Pred Labels):\")\n",
    "                 print(raw_cm)\n",
    "\n",
    "\n",
    "                 # Cost matrix for Hungarian algorithm: maximize overlap = minimize negative counts\n",
    "                 # Need to ensure cost matrix aligns with labels properly\n",
    "                 # We want to map predicted clusters (columns of raw_cm potentially) to true labels (rows)\n",
    "                 cost_matrix = -raw_cm # Shape: (n_true_labels, n_pred_labels_observed) -> Make square if needed?\n",
    "\n",
    "                 # Pad cost matrix if not square (assign low cost to non-matched)\n",
    "                 n_rows, n_cols = cost_matrix.shape\n",
    "                 if n_rows > n_cols: # More true labels than predicted clusters found\n",
    "                     padding = np.zeros((n_rows, n_rows - n_cols))\n",
    "                     cost_matrix = np.hstack((cost_matrix, padding))\n",
    "                 elif n_cols > n_rows: # More predicted clusters than true labels\n",
    "                     padding = np.zeros((n_cols - n_rows, n_cols))\n",
    "                     cost_matrix = np.vstack((cost_matrix, padding))\n",
    "\n",
    "\n",
    "                 row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "                 remapping_dict = {}\n",
    "                 print(\"  Optimal Assignment (True Label Index -> Assigned Original Cluster Index):\")\n",
    "                 # Need to map row_ind back to true color int, and col_ind back to original cluster label\n",
    "                 original_cluster_labels_list = matrix_calc_labels_pred # Use the labels actually present\n",
    "\n",
    "                 for r, c in zip(row_ind, col_ind):\n",
    "                     # Ensure indices are within the bounds of the *potentially padded* cost matrix\n",
    "                     if r < len(matrix_calc_labels_true): # r corresponds to true target int index\n",
    "                         target_plot_int = matrix_calc_labels_true[r]\n",
    "                         true_color_name = plot_int_to_true_color.get(target_plot_int, f\"Unknown Target {target_plot_int}\")\n",
    "\n",
    "                         if c < len(original_cluster_labels_list): # c corresponds to *observed* predicted cluster index\n",
    "                              original_cluster_label = original_cluster_labels_list[c]\n",
    "                              print(f\"    True '{true_color_name}' (Target Int {target_plot_int}) best matches Original Cluster Label {original_cluster_label}\")\n",
    "                              remapping_dict[original_cluster_label] = target_plot_int\n",
    "                         # else:\n",
    "                         #     # This column index came from padding, means this true label wasn't assigned\n",
    "                         #     print(f\"    True '{true_color_name}' (Target Int {target_plot_int}) has no matching cluster (assigned to padding).\")\n",
    "\n",
    "                 print(f\"  Cluster Remapping Dict (Original Cluster -> Target Plot Int): {remapping_dict}\")\n",
    "                 # Apply mapping, handle unmapped clusters (assign -1 or a default)\n",
    "                 unmapped_value = -1 # Or perhaps max(remapping_dict.values()) + 1\n",
    "                 data_with_clusters['Cluster_Remapped'] = data_with_clusters['Cluster'].map(remapping_dict).fillna(unmapped_value).astype(int)\n",
    "\n",
    "\n",
    "                 print(\"\\n--- Generating Confusion Matrix (using OPTIMALLY REMAPPED cluster labels) ---\")\n",
    "                 plot_output_dir = os.path.join(OUTPUT_DIR, \"plots_remapped_optimal\")\n",
    "                 plot_color_confusion_matrix(data_with_clusters, cluster_col='Cluster_Remapped', true_color_col='Color Preference', output_dir=plot_output_dir)\n",
    "                 original_plot_name = f\"confusion_matrix_Cluster_Remapped_vs_Color Preference.png\"\n",
    "                 new_plot_name = f\"confusion_matrix_OPTIMAL_REORDERED_{FEATURE_DISTANCE_METHOD}_{CLUSTER_METHOD}_k{N_CLUSTERS}.png\"\n",
    "                 try:\n",
    "                     if not os.path.exists(plot_output_dir): os.makedirs(plot_output_dir)\n",
    "                     if os.path.exists(os.path.join(plot_output_dir, original_plot_name)):\n",
    "                          os.rename(os.path.join(plot_output_dir, original_plot_name), os.path.join(plot_output_dir, new_plot_name))\n",
    "                          print(f\" Renamed plot to {new_plot_name}\")\n",
    "                     else: print(f\" Plot file not found for renaming: {original_plot_name}\")\n",
    "                 except Exception as e_mv: print(f\" Could not rename remapped plot: {e_mv}\")\n",
    "\n",
    "             except ImportError: print(\"\\n--- Cluster Remapping Failed: scipy.optimize not found. ---\"); plot_original = True\n",
    "             except Exception as e:\n",
    "                  print(f\"Could not remap labels or generate remapped plot: {repr(e)}\");\n",
    "                  import traceback\n",
    "                  traceback.print_exc()\n",
    "                  plot_original = True\n",
    "\n",
    "             # Fallback to plotting original if remapping failed or flag set\n",
    "             if 'plot_original' in locals() and plot_original:\n",
    "                 print(\" Plotting confusion matrix with original KMeans labels due to remapping failure or issue.\")\n",
    "                 plot_output_dir = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "                 plot_color_confusion_matrix(data_with_clusters, cluster_col='Cluster', true_color_col='Color Preference', output_dir=plot_output_dir)\n",
    "\n",
    "        else: print(\"\\nSkipping confusion matrix plot (No valid clusters found or data empty).\")\n",
    "\n",
    "\n",
    "    print(\"\\nPipeline finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9fbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f96716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc7cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eecc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031301e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e79c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e8aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc70f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1c151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9307630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0aae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180660cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afcdf10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cec628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaaf64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f6746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995850d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aa6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c3fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef115837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561d866a",
   "metadata": {},
   "source": [
    "**Overall Purpose:**\n",
    "\n",
    "The script simulates multiple agents exploring a structured, colored graph. Each agent has a specific color preference and aims to find a certain number of nodes matching that preference within a step limit. After the simulations, the script clusters the agents' traversal paths based on features derived from these paths (specifically \"Bag-of-Colors\") and evaluates how well these clusters correspond to the agents' original color preferences. It focuses on a specific configuration using a structured graph *without* noise and a deterministic agent strategy.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1.  **Imports:** Imports necessary libraries for graph manipulation (`networkx`), plotting (`matplotlib`, `seaborn`), data handling (`pandas`, `numpy`), feature extraction and clustering (`sklearn`), distance metrics and hierarchical clustering (`scipy`), optimization for label remapping (`scipy.optimize`), timing (`time`), file operations (`os`), and warnings management (`warnings`).\n",
    "\n",
    "2.  **`Agent` Class (V5.1):**\n",
    "    * Defines an agent that traverses the graph.\n",
    "    * **Initialization:** Takes a start node, a `color_preference`, a `target_preferred_count` (goal), and `max_steps`.\n",
    "    * **`find_next_node`:** Implements the core movement logic. It uses a **deterministic preference strategy**: If unvisited neighbors matching the `color_preference` exist, it *always* chooses randomly among them. Otherwise, it chooses randomly from *any* other unvisited neighbors. Includes an explicit loop fix for robustness.\n",
    "    * **`traverse_graph`:** Simulates the agent's walk, calling `find_next_node` repeatedly until the goal is met, `max_steps` are reached, or the agent gets stuck. It records the path taken, visited node attributes (color, shape, shiny), and counts the preferred nodes found.\n",
    "\n",
    "3.  **`generate_structured_colored_graph` Function:**\n",
    "    * Creates a `networkx` graph where nodes are assigned colors, shapes, and shininess attributes.\n",
    "    * The coloring is structured: nodes are primarily assigned to color \"regions,\" but a `noise_level` parameter (set to 0.0 in the main block) allows for some nodes to randomly get a different color.\n",
    "    * Edges are added randomly based on an `edge_probability`.\n",
    "\n",
    "4.  **`simulate_agents` Function:**\n",
    "    * Orchestrates the simulation runs.\n",
    "    * Takes a graph, number of agents, traversals per agent, seeds, etc.\n",
    "    * Initializes multiple `Agent` instances with random start nodes and color preferences.\n",
    "    * Runs `traverse_graph` for each agent traversal.\n",
    "    * Collects detailed results (path, steps, found count, path properties, visited attributes) into a `pandas.DataFrame`.\n",
    "\n",
    "5.  **Feature Engineering & Distance Functions:**\n",
    "    * `jaccard_distance`: Calculates Jaccard distance between sets (likely node paths).\n",
    "    * `extract_boc_features`: Extracts **Bag-of-Colors (BoC)** features from the sequence of colors visited by each agent using `CountVectorizer`.\n",
    "    * `extract_combined_features`: Combines BoC with numerical features.\n",
    "    * `calculate_distance_matrix`: Calculates distance matrices (e.g., using Jaccard) or extracts feature matrices (BoC, combined) based on the specified method.\n",
    "\n",
    "6.  **`cluster_and_evaluate` Function:**\n",
    "    * Performs clustering on the simulation data (grouped by simulation seed).\n",
    "    * Takes the simulation DataFrame, number of clusters (`n_clusters`), feature/distance method, and clustering algorithm (`cluster_method`) as input.\n",
    "    * Calculates distances/features for each seed's data.\n",
    "    * Applies the chosen clustering algorithm (e.g., 'kmeans', 'ward').\n",
    "    * Evaluates the clustering results using:\n",
    "        * **Silhouette Score:** Measures cluster cohesion and separation.\n",
    "        * **Adjusted Rand Index (ARI) & Normalized Mutual Information (NMI):** Compare cluster assignments to the 'ground truth' (agent's `Color Preference`).\n",
    "        * **Cluster Color Purity/Percentage:** Calculates the distribution of visited colors within each cluster.\n",
    "    * Returns a summary DataFrame of evaluation metrics and the original DataFrame with added cluster labels.\n",
    "\n",
    "7.  **Plotting Functions:**\n",
    "    * `plot_colored_graph`: Visualizes the generated graph structure. (Not called in the main block).\n",
    "    * `plot_color_confusion_matrix`: Creates a heatmap comparing the true color preferences against the assigned cluster labels.\n",
    "\n",
    "8.  **Main Execution Block (`if __name__ == \"__main__\":`)**\n",
    "    * **Sets Parameters:** Defines graph size, colors, simulation runs, agent goals (`TARGET_PREFERRED_COUNT`, `MAX_STEPS`), graph structure (`GRAPH_NOISE_LEVEL = 0.0`), and clustering parameters (`FEATURE_DISTANCE_METHOD = 'boc'`, `CLUSTER_METHOD = 'kmeans'`, `N_CLUSTERS = 5`).\n",
    "    * **Runs Workflow:**\n",
    "        1.  Loops through multiple simulation seeds.\n",
    "        2.  Generates a structured graph *without noise*.\n",
    "        3.  Runs `simulate_agents` using the Agent V.\n",
    "        4.  Concatenates results.\n",
    "        5.  Calls `cluster_and_evaluate` using BoC features and KMeans.\n",
    "        6.  Prints and saves the evaluation summary.\n",
    "        7.  **Optimal Cluster Label Remapping:** Attempts to reorder the KMeans cluster labels using the Hungarian algorithm (`linear_sum_assignment`) to best align with the true color preferences, making the confusion matrix more interpretable.\n",
    "        8.  Generates and saves the confusion matrix plot using the *remapped* cluster labels (or original labels if remapping fails).\n",
    "    * **Output:** Creates a directory containing a CSV summary of clustering results and a confusion matrix PNG image.\n",
    "\n",
    "**In summary, the script sets up and runs an experiment to test if KMeans clustering applied to Bag-of-Colors features derived from agent traversals (using a deterministic preference strategy on a noise-free structured graph) can effectively group agents based on their intended color preference. It includes an advanced step to optimally align cluster labels for clearer visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254efe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
